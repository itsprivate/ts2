{
  "@type": "NewsArticle",
  "identifier": "2022--06--22--en--showhn--HackerNews--NewsArticle--31837307",
  "url": "https://news.ycombinator.com/item?id=31837307",
  "headline": "Show HN: Data Diff – compare tables of any size across databases",
  "publisher": {
    "@type": "Organization",
    "name": "HackerNews",
    "url": "https://news.ycombinator.com",
    "logo": "https://hn.buzzing.cc/avatar.png"
  },
  "description": "Gleb, Alex, Erez and Simon here – we are building an open-source tool for comparing data within and across databases at any scale. The repo is at https://github.com/datafold/data-diff, and our home page is https://datafold.com/.<p>As a company, Datafold builds tools for data engineers to automate the most tedious and error-prone tasks falling through the cracks of the modern data stack, such as data testing and lineage. We launched two years ago with a tool for regression-testing changes to ETL code https://news.ycombinator.com/item?id=24071955. It compares the produced data before and after the code change and shows the impact on values, aggregate metrics, and downstream data applications.<p>While working with many customers on improving their data engineering experience, we kept hearing that they needed to diff their data across databases to validate data replication between systems.<p>There were 3 main use cases for such replication:<p>(1) To perform analytics on transactional data in an OLAP engine (e.g. PostgreSQL &gt; Snowflake)\n(2) To migrate between transactional stores (e.g. MySQL &gt; PostgreSQL)\n(3) To leverage data in a specialized engine (e.g. PostgreSQL &gt; ElasticSearch).<p>Despite multiple vendors (e.g., Fivetran, Stitch) and open-source products (Airbyte, Debezium) solving data replication, there was no tooling for validating the correctness of such replication. When we researched how teams were going about this, we found that most have been either:<p>Running manual checks: e.g., starting with COUNT(*) and then digging into the discrepancies, which often took hours to pinpoint the inconsistencies.\nUsing distributed MPP engines such as Spark or Trino to download the complete datasets from both databases and then comparing them in memory – an expensive process requiring complex infrastructure.<p>Our users wanted a tool that could:<p>(1) Compare datasets quickly (seconds/minutes) at a large (millions/billions of rows) scale across different databases (2) Have minimal network IO and database workload overhead. (3) Provide straightforward output: basic stats and what rows are different. (4) Be embedded into a data orchestrator such as Airflow to run right after the replication process.<p>So we built Data Diff as an open-source package available through pip. Data Diff can be run in a CLI or wrapped into any data orchestrator such as Airflow, Dagster, etc.<p>To solve for speed at scale with minimal overhead, Data Diff relies on checksumming the data in both databases and uses binary search to identify diverging records. That way, it can compare arbitrarily large datasets in logarithmic time and IO – only transferring a tiny fraction of the data over the network. For example, it can diff tables with 25M rows in ~10s and 1B+ rows in ~5m across two physically separate PostgreSQL databases while running on a typical laptop.<p>We've launched this tool under the MIT license so that any developer can use it, and to encourage contributions of other database connectors. We didn't want to charge engineers for such a fundamental use case. We make money by charging a license fee for advanced solutions such as column-level data lineage, CI workflow automation, and ML-powered alerts.",
  "keywords": [
    "Show HN"
  ],
  "genre": "Show HN",
  "author": {
    "@type": "Person",
    "name": "hichkaker",
    "url": "https://news.ycombinator.com/user?id=hichkaker"
  },
  "discussionUrl": "https://news.ycombinator.com/item?id=31837307",
  "sameAs": "https://news.ycombinator.com/item?id=31837307",
  "dateCreated": "2022-06-22T15:48:41.785Z",
  "datePublished": "2022-06-22T15:28:04.000Z",
  "dateModified": "2022-06-22T15:48:41.785Z",
  "interactionStatistic": [
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "LikeAction"
      },
      "userInteractionCount": 14
    },
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "CommentAction"
      },
      "userInteractionCount": 0
    }
  ],
  "headline_zh-Hans": "Show HN: 数据差异--跨数据库比较任何大小的表\n",
  "headline_zh-Hant": "Show HN: 數據差異--跨數據庫比較任何大小的表\n",
  "@context": [
    "https://schema.org",
    {
      "@vocab": "http://schema.org/",
      "@language": "en",
      "headline_zh-Hans": {
        "@id": "headline",
        "@language": "zh-Hans"
      },
      "headline_zh-Hant": {
        "@id": "headline",
        "@language": "zh-Hant"
      },
      "@version": 1.1,
      "description_zh-Hans": {
        "@id": "description",
        "@language": "zh-Hans"
      },
      "description_zh-Hant": {
        "@id": "description",
        "@language": "zh-Hant"
      }
    }
  ],
  "description_zh-Hans": "我是Gleb、Alex、Erez和Simon--我们正在建立一个开源工具，用于比较任何规模的数据库内部和跨数据库的数据。<p>作为一家公司，Datafold为数据工程师建立了一些工具，以自动处理现代数据堆栈中最繁琐和最容易出错的任务，如数据测试和线程。我们在两年前推出了一个工具，用于回归测试ETL代码的变化，https://news.ycombinator.com/item?id=24071955。<p>在与许多客户合作改善他们的数据工程体验时，我们不断听到他们需要在数据库之间进行数据差异，以验证系统之间的数据复制。<p>这种复制有3个主要用例：<p>(1) 在OLAP引擎（如PostgreSQL &gt; Snowflake）中对事务性数据进行分析\n(2) 在事务性存储之间进行迁移（例如，MySQL &gt; PostgreSQL）。\n(<p>尽管有多家供应商（如Fivetran、Stitch）和开源产品（Airbyte、Debezium）解决了数据复制的问题，但没有任何工具可以验证这种复制的正确性。当我们研究团队如何去做这件事时，我们发现大多数都是：<p>运行人工检查：例如，从COUNT(*)开始，然后挖掘差异，这往往要花几个小时才能找出不一致的地方。\n使用分布式MPP引擎，如Spark或Trino，从两个数据库中下载完整的数据集，然后在内存中进行比较，这是一个昂贵的过程，需要复杂的基础设施。<p>我们的用户希望有一个工具能够：<p>(1) 在不同数据库中快速（秒/分钟）比较大型（数百万/数十亿行）的数据集 (2) 具有最小的网络IO和数据库工作负载。(3) 提供直接的输出：基本统计数字和哪些行是不同的。(4) 嵌入到数据协调器中，如Airflow，以便在复制过程后立即运行。<p>因此，我们将Data Diff作为一个开源包，通过pip提供。Data Diff可以在CLI中运行，也可以封装到任何数据协调器中，如Airflow、Dagster等。<p>为了以最小的开销解决大规模的速度问题，Data Diff依赖于对两个数据库的数据进行校验，并使用二进制搜索来识别不同的记录。这样，它可以在对数时间和IO中比较任意大的数据集--只在网络上传输极小部分的数据。例如，它可以在一个典型的笔记本电脑上运行时，在两个物理上独立的PostgreSQL数据库中，在~10s内对有2500万行的表进行比较，在~5m内对有1B+行的表进行比较。<p>我们在MIT许可下推出这个工具，以便任何开发者都可以使用它，并鼓励其他数据库连接器的贡献。我们不想为这样一个基本的用例向工程师收费。我们通过对高级解决方案收取许可费来赚钱，如列级数据线、CI工作流自动化和ML驱动的警报。\n",
  "description_zh-Hant": "我是Gleb、Alex、Erez和Simon--我們正在建立一個開源工具，用於比較任何規模的數據庫內部和跨數據庫的數據。<p>作為一家公司，Datafold為數據工程師建立了一些工具，以自動處理現代數據堆棧中最繁瑣和最容易出錯的任務，如數據測試和線程。我們在兩年前推出了一個工具，用於迴歸測試ETL代碼的變化，https://news.ycombinator.com/item?id=24071955。<p>在與許多客戶合作改善他們的數據工程體驗時，我們不斷聽到他們需要在數據庫之間進行數據差異，以驗證系統之間的數據複製。<p>這種複製有3個主要用例：<p>(1) 在OLAP引擎（如PostgreSQL &gt; Snowflake）中對事務性數據進行分析\n(2) 在事務性存儲之間進行遷移（例如，MySQL &gt; PostgreSQL）。\n(<p>儘管有多家供應商（如Fivetran、Stitch）和開源產品（Airbyte、Debezium）解決了數據複製的問題，但沒有任何工具可以驗證這種複製的正確性。當我們研究團隊如何去做這件事時，我們發現大多數都是：<p>運行人工檢查：例如，從COUNT(*)開始，然後挖掘差異，這往往要花幾個小時才能找出不一致的地方。\n使用分佈式MPP引擎，如Spark或Trino，從兩個數據庫中下載完整的數據集，然後在內存中進行比較，這是一個昂貴的過程，需要複雜的基礎設施。<p>我們的用戶希望有一個工具能夠：<p>(1) 在不同數據庫中快速（秒/分鐘）比較大型（數百萬/數十億行）的數據集 (2) 具有最小的網絡IO和數據庫工作負載。(3) 提供直接的輸出：基本統計數字和哪些行是不同的。(4) 嵌入到數據協調器中，如Airflow，以便在複製過程後立即運行。<p>因此，我們將Data Diff作為一個開源包，通過pip提供。Data Diff可以在CLI中運行，也可以封裝到任何數據協調器中，如Airflow、Dagster等。<p>為了以最小的開銷解決大規模的速度問題，Data Diff依賴於對兩個數據庫的數據進行校驗，並使用二進制搜索來識別不同的記錄。這樣，它可以在對數時間和IO中比較任意大的數據集--只在網絡上傳輸極小部分的數據。例如，它可以在一個典型的筆記本電腦上運行時，在兩個物理上獨立的PostgreSQL數據庫中，在~10s內對有2500萬行的表進行比較，在~5m內對有1B+行的表進行比較。<p>我們在MIT許可下推出這個工具，以便任何開發者都可以使用它，並鼓勵其他數據庫連接器的貢獻。我們不想為這樣一個基本的用例向工程師收費。我們通過對高級解決方案收取許可費來賺錢，如列級數據線、CI工作流自動化和ML驅動的警報。\n"
}