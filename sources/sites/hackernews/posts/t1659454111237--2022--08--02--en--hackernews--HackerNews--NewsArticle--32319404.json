{
  "@type": "NewsArticle",
  "identifier": "2022--08--02--en--hackernews--HackerNews--NewsArticle--32319404",
  "url": "https://news.ycombinator.com/item?id=32319404",
  "headline": "Launch HN: DeploySentinel (YC S22) – End-to-end tests that don't flake",
  "publisher": {
    "@type": "Organization",
    "name": "HackerNews",
    "url": "https://news.ycombinator.com",
    "logo": "https://hn.buzzing.cc/avatar.png"
  },
  "description": "Hi HN, Michael and Warren here - cofounders of DeploySentinel (<a href=\"https://deploysentinel.com\" rel=\"nofollow\">https://deploysentinel.com</a>). We make end-to-end testing easier and more reliable.<p>At my last job, it dawned upon me how many production incidents and unhappy customers could have been avoided with more test automation - “an ounce of prevention is worth a pound of cure”. However, it wasn’t clear that you can get prevention for just an ounce. Our teams ramped up on investment into testing, especially end-to-end tests (via spinning up a headless browser in CI and testing as an end user), but it quickly became clear that these were incredibly expensive to build and maintain, especially as test suite and application complexity grow. When we asked around other engineering teams from different companies, we consistently heard how time-intensive test maintenance was.<p>The worst part of end to end tests is when they fail occasionally in CI but never locally—a heisenbug in your test code, or what’s usually referred to as a flaky test. The conventional way to debug such an issue is to replay a video of your CI’s test browser, stepping between video frames to try to parse what could be happening under the hood. Otherwise, your CI is just a complete black box.<p>Anyone that finds this story familiar can probably attest to days spent trying to debug an issue like this, possibly losing some hair in the process, and “resolving” it in the end by just deleting the test and regaining their sanity. Some teams even try to put front-end monitoring tools for production into their CI process, only to realize they aren’t able to handle recording hundreds of test actions executed by a machine over just a few seconds.<p>After realizing how painful debugging these tests could be, we started putting together a debugger that can help developers pinpoint issues, more like how you debug issues locally. Teams have told us there’s a night and day difference between trying to debug test failures with just video, and having a tool that can finally tell them what’s happening in their CI browser, with the same information they’re used to having in their browser’s devtools.<p>We give you the ability to inspect DOM snapshots, network events, and console logs for any step taken in a Cypress test running in CI, to give more insight into why a particular test might be failing. It’s like Fullstory/LogRocket, but for CI failures instead of production bugs. (We’re starting with Cypress tests, with plans to extend further.)<p>Our tool integrates with Cypress via their plugin API, so we’re able to plug in and record tests in CI with just an NPM install and 2 lines of code. From there we’re able to hook into Cypress/Mocha events to capture everything happening within the test runner (ex. when a test is starting, when a command is fired, when an element is found, etc.) as well as open a debugger protocol port with the browser to listen for network and console events. While a test suite is running, the debugger is consistently collecting what’s happening during a test run, and uploads the information (minus user-configured censored events) after every test completes.<p>While this may sound similar to shoving a LogRocket/FullStory into your test suite, there’s actually quite a few differences. The most practical one is that those tools typically have a low rate limit that work well for human traffic interacting with web apps at human speeds, but break when dealing with parallelized test runner traffic interacting with web app at machine speeds. Other minor details revolve around us associating replays with test metadata as opposed to user metadata, having full access to all network requests/console messages emitted within a test at the browser level, and us indexing playback information based on test commands rather than timestamp (time is an unreliable concept in tests!).<p>Once a test fails, a Github PR comment is created and an engineer can immediately access our web app to start debugging their test failure. Alternatively, they can check our web dashboard as well. Instead of playing a video of the failure in slow motion to understand the issue, an engineer can step through the test command-by-command, inspect the DOM with their browser inspect element tool at any point, view what elements the test interacted with, if any console messages were emitted during the action, or take a look at every network request made along with HTTP error codes or browser network error messages.<p>Typically with this kind of information, engineers can quickly find out if they have a network-based race condition, a console warning emitted in their frontend, a server-side bug, or a test failure from an edge case triggered by randomly generated test data.<p>We dream of a world where applications have minimal bugs, happy customers, built with engineering teams that don’t see testing as an expensive chore! Although the first pain we’re addressing is tests that fail in CI, we’re working on a bunch of things beyond that, including the second biggest issue in testing: test runtime length.<p>We have a free trial available for you to try out with your own tests, along with a few live demos of what our debugger looks like on an example test. You can get started here: <a href=\"https://deploysentinel.com/\" rel=\"nofollow\">https://deploysentinel.com/</a><p>We’re looking forward to hearing everyone else’s experiences with end to end tests, and what you think of what we’re doing!",
  "keywords": [],
  "author": {
    "@type": "Person",
    "name": "mikeshi42",
    "url": "https://news.ycombinator.com/user?id=mikeshi42"
  },
  "discussionUrl": "https://news.ycombinator.com/item?id=32319404",
  "sameAs": "https://news.ycombinator.com/item?id=32319404",
  "dateCreated": "2022-08-02T15:28:31.237Z",
  "datePublished": "2022-08-02T15:01:29.000Z",
  "dateModified": "2022-08-02T15:28:31.237Z",
  "interactionStatistic": [
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "LikeAction"
      },
      "userInteractionCount": 4
    },
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "CommentAction"
      },
      "userInteractionCount": 2
    }
  ],
  "headline_zh-Hans": "启动HN：DeploySentinel（YC S22）--不会剥落的端到端测试\n",
  "headline_zh-Hant": "啟動HN：DeploySentinel（YC S22）--不會剝落的端到端測試\n",
  "@context": [
    "https://schema.org",
    {
      "@vocab": "http://schema.org/",
      "@language": "en",
      "headline_zh-Hans": {
        "@id": "headline",
        "@language": "zh-Hans"
      },
      "headline_zh-Hant": {
        "@id": "headline",
        "@language": "zh-Hant"
      },
      "@version": 1.1,
      "description_zh-Hans": {
        "@id": "description",
        "@language": "zh-Hans"
      },
      "description_zh-Hant": {
        "@id": "description",
        "@language": "zh-Hant"
      }
    }
  ],
  "description_zh-Hans": "嗨，HN，我是迈克尔和沃伦--DeploySentinel（<a href=\"https://deploysentinel.com\" rel=\"nofollow\">https://deploysentinel.com</a>）的联合创始人。<p>在我的上一份工作中，我突然意识到如果有更多的测试自动化，那么有多少生产事故和不满意的客户是可以避免的 - \"一盎司的预防胜过一磅的治疗\"。然而，不清楚你是否可以用一盎司来获得预防。我们的团队加大了对测试的投资，特别是端到端的测试（通过在CI中启动无头浏览器，并作为终端用户进行测试），但很快就发现这些测试的构建和维护成本非常高，特别是随着测试套件和应用程序复杂性的增加。当我们询问不同公司的其他工程团队时，我们一直听说测试维护是多么的费时费力。<p>端到端测试最糟糕的部分是，它们在CI中偶尔会失败，但在本地却不会，这是你的测试代码中的heisenbug，或者通常被称作 \"闪烁 \"测试。调试这种问题的常规方法是重放你的CI的测试浏览器的视频，在视频帧之间步步紧逼，以尝试解析可能发生在引擎盖下的问题。<p>任何觉得这个故事很熟悉的人可能会证明，他们花了好几天时间来调试这样的问题，在这个过程中可能会失去一些头发，最后通过删除测试来 \"解决 \"这个问题，恢复了他们的理智。一些团队甚至试图将生产的前端监控工具放到他们的CI流程中，却发现他们无法处理记录机器在短短几秒钟内执行的数百个测试动作。<p>在意识到调试这些测试是多么痛苦之后，我们开始将一个可以帮助开发人员确定问题的调试器放在一起，更像是你在本地调试问题的方式。团队告诉我们，在尝试用视频来调试测试故障时，和拥有一个工具来告诉他们在CI浏览器中发生了什么，以及他们习惯于在浏览器的devtools中获得相同的信息时，两者之间有着天壤之别。<p>我们让你能够检查DOM快照、网络事件以及在CI中运行的Cypress测试的任何步骤的控制台日志，以便更深入地了解某个特定测试可能失败的原因。这就像Fullstory/LogRocket，但针对的是CI故障而不是生产错误。(我们从Cypress测试开始，计划进一步扩展。) <p>我们的工具通过插件API与Cypress集成，因此我们能够插入并记录CI中的测试，只需安装NPM和两行代码。在那里，我们能够与Cypress/Mocha事件挂钩，以捕获测试运行器中发生的一切（例如，当测试开始时，当命令被触发时，当发现一个元素时，等等），以及与浏览器打开一个调试器协议端口，以监听网络和控制台事件。当测试套件运行时，调试器一直在收集测试运行期间发生的事情，并在每次测试完成后上传这些信息（减去用户配置的审查事件）。<p>虽然这听起来可能类似于将LogRocket/FullStory塞进你的测试套件，但实际上有不少区别。最实际的一点是，这些工具通常具有较低的速率限制，对于以人类速度与 Web 应用程序进行交互的人类流量来说效果很好，但在处理以机器速度与 Web 应用程序进行交互的并行测试运行器流量时就会出现问题。其他次要的细节围绕着我们将回放与测试元数据而不是用户元数据相关联，在浏览器级别完全访问测试中发出的所有网络请求/控制台消息，以及我们基于测试命令而不是时间戳对回放信息进行索引（时间在测试中是一个不可靠的概念！）。<p>一旦测试失败，将创建一个Github PR评论，工程师可以立即访问我们的网络应用，开始调试他们的测试失败。另外，他们也可以查看我们的网络仪表板。工程师不需要通过慢动作播放失败的视频来了解问题，而是可以逐个命令地进行测试，在任何时候用浏览器的检查元素工具检查DOM，查看测试与哪些元素交互，在操作过程中是否有任何控制台信息发出，或者查看每个网络请求以及HTTP错误代码或 \n",
  "description_zh-Hant": "嗨，HN，我是邁克爾和沃倫--DeploySentinel（<a href=\"https://deploysentinel.com\" rel=\"nofollow\">https://deploysentinel.com</a>）的聯合創始人。<p>在我的上一份工作中，我突然意識到如果有更多的測試自動化，那麼有多少生產事故和不滿意的客戶是可以避免的 - \"一盎司的預防勝過一磅的治療\"。然而，不清楚你是否可以用一盎司來獲得預防。我們的團隊加大了對測試的投資，特別是端到端的測試（通過在CI中啟動無頭瀏覽器，並作為終端用戶進行測試），但很快就發現這些測試的構建和維護成本非常高，特別是隨著測試套件和應用程序複雜性的增加。當我們詢問不同公司的其他工程團隊時，我們一直聽說測試維護是多麼的費時費力。<p>端到端測試最糟糕的部分是，它們在CI中偶爾會失敗，但在本地卻不會，這是你的測試代碼中的heisenbug，或者通常被稱作 \"閃爍 \"測試。調試這種問題的常規方法是重放你的CI的測試瀏覽器的視頻，在視頻幀之間步步緊逼，以嘗試解析可能發生在引擎蓋下的問題。<p>任何覺得這個故事很熟悉的人可能會證明，他們花了好幾天時間來調試這樣的問題，在這個過程中可能會失去一些頭髮，最後通過刪除測試來 \"解決 \"這個問題，恢復了他們的理智。一些團隊甚至試圖將生產的前端監控工具放到他們的CI流程中，卻發現他們無法處理記錄機器在短短几秒鐘內執行的數百個測試動作。<p>在意識到調試這些測試是多麼痛苦之後，我們開始將一個可以幫助開發人員確定問題的調試器放在一起，更像是你在本地調試問題的方式。團隊告訴我們，在嘗試用視頻來調試測試故障時，和擁有一個工具來告訴他們在CI瀏覽器中發生了什麼，以及他們習慣於在瀏覽器的devtools中獲得相同的信息時，兩者之間有著天壤之別。<p>我們讓你能夠檢查DOM快照、網絡事件以及在CI中運行的Cypress測試的任何步驟的控制檯日誌，以便更深入地瞭解某個特定測試可能失敗的原因。這就像Fullstory/LogRocket，但針對的是CI故障而不是生產錯誤。(我們從Cypress測試開始，計劃進一步擴展。) <p>我們的工具通過插件API與Cypress集成，因此我們能夠插入並記錄CI中的測試，只需安裝NPM和兩行代碼。在那裡，我們能夠與Cypress/Mocha事件掛鉤，以捕獲測試運行器中發生的一切（例如，當測試開始時，當命令被觸發時，當發現一個元素時，等等），以及與瀏覽器打開一個調試器協議端口，以監聽網絡和控制檯事件。當測試套件運行時，調試器一直在收集測試運行期間發生的事情，並在每次測試完成後上傳這些信息（減去用戶配置的審查事件）。<p>雖然這聽起來可能類似於將LogRocket/FullStory塞進你的測試套件，但實際上有不少區別。最實際的一點是，這些工具通常具有較低的速率限制，對於以人類速度與 Web 應用程序進行交互的人類流量來說效果很好，但在處理以機器速度與 Web 應用程序進行交互的並行測試運行器流量時就會出現問題。其他次要的細節圍繞著我們將回放與測試元數據而不是用戶元數據相關聯，在瀏覽器級別完全訪問測試中發出的所有網絡請求/控制檯消息，以及我們基於測試命令而不是時間戳對回放信息進行索引（時間在測試中是一個不可靠的概念！）。<p>一旦測試失敗，將創建一個Github PR評論，工程師可以立即訪問我們的網絡應用，開始調試他們的測試失敗。另外，他們也可以查看我們的網絡儀表板。工程師不需要通過慢動作播放失敗的視頻來了解問題，而是可以逐個命令地進行測試，在任何時候用瀏覽器的檢查元素工具檢查DOM，查看測試與哪些元素交互，在操作過程中是否有任何控制檯信息發出，或者查看每個網絡請求以及HTTP錯誤代碼或 \n"
}