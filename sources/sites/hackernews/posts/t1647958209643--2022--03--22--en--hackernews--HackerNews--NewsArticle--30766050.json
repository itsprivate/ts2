{
  "@type": "NewsArticle",
  "identifier": "2022--03--22--en--hackernews--HackerNews--NewsArticle--30766050",
  "url": "https://news.ycombinator.com/item?id=30766050",
  "headline": "Launch HN: Reality Defender (YC W22) – Deepfake Detection Platform",
  "publisher": {
    "@type": "Organization",
    "name": "HackerNews",
    "url": "https://news.ycombinator.com",
    "logo": "https://hn.buzzing.cc/avatar.png"
  },
  "description": "Hi HN, we’re Ben, Gaurav and Ali from Reality Defender (<a href=\"https://www.realitydefender.ai\" rel=\"nofollow\">https://www.realitydefender.ai</a>). We help companies, governments, and journalists determine if media is real or fake, focusing on audio, video and image manipulation. Our API and web app provide real-time scanning, risk scoring, and PDF report cards.<p>Recent advancements in machine learning make it possible to create images, videos and audio of real people saying and doing things they never said or did. The recent spread of this technology has enabled anyone to create highly realistic deepfakes. Although some deepfakes are detectable to the eye by experienced observers who look closely, many people either don’t have experience or are not always looking closely—and of course the technology is only continuing to improve. This marks a leap in the ability of bad actors to distort reality, jeopardizing financial transactions, personal and brand reputations, public opinion, and even national security.<p>We are a team with PhD and Master degrees from Harvard, NYU and UCLA in data science. Between us, we have decades of experience at Goldman Sachs, Google, CIA, FDIC, Dept of Defense and Harvard University Applied Research at the intersection of machine learning and cybersecurity. But our current work began with a rather unlikely project: we tried to duplicate Deepak Chopra. We were working with him to build a realistic deepfake that would allow users to have a real-time conversation with “Digital Deepak” from their iPhones. Creating the Deepak deepfake was surprisingly simple and the result was so alarmingly realistic that we immediately began looking for models that could help users tell a synthetic version from the real thing.<p>We did not find a reliable solution. Frustrated that we’d already spent a week on something we thought would take our coffee break, we doubled down and set out to build our own model that could detect manipulated media.<p>After investigating, we learned why a consistently accurate solution didn’t exist. Companies (including Facebook and Microsoft) were trying to build their own silver-bullet, single-model detection methods—or, as we call it, &quot;one model to rule them all.&quot; In our view, this approach will not work because adversaries and the underlying technologies are constantly evolving. For this same reason there will never be a single model to solve anti-virus, malware, etc.<p>We believe that any serious solution to this problem requires a “multi-model'' approach that integrates the best deepfake detection algorithms into an aggregate &quot;model of models.&quot; So we trained an ensemble of deep-learning detection models, each of which focuses on its own feature, and then combined the scores.<p>We challenged ourselves to build a scalable solution that integrates the best of our deepfake detection models with models from our collaborators (Microsoft, UC Berkeley, Harvard). We began with a web app proof of concept, and quickly received hundreds of requests for access from governments, companies, and researchers.<p>Our first users turned to our platform for some deepfake scenarios ranging from bad to outright scary: Russian disinformation directed at Ukraine and the West; audio mimicking a bank executive requesting a wire transfer; video of Malaysia’s government leadership behaving scandalously; pornography where participants make themselves appear younger; dating profiles with AI-generated pro pics. All of these, needless to say, are completely fake!<p>As with computer viruses, deepfakes will continue evolving to circumvent current security measures. New deepfake detection techniques must be as iterative as the generation methods. Our solution not only accepts that, but embraces it. We quickly onboard, test, and tune third party models for integration into our model stack, where they can then be accessed via our web app and API. Our mission has attracted dozens of researchers who contribute their work for testing and tuning, and we’ve come up with an interesting business model for working together: when their models meet our baseline scores, we provide a revenue share for as long as they continue to perform on our platform. (If you’re interested in participating, we’d love to hear from you!)<p>We have continued to scale our web app and launched an API that we are rolling out to pilot customers. Currently the most popular use cases are: KYC onboarding fraud detection and voice fraud detection (ie. banks, marketplaces); and user-generated deepfake content moderation (ie. social media, dating platforms, news and government organizations).<p>We are currently testing a monthly subscription to scan a minimum of 250 media assets per month. We offer a 30 day pilot that converts into a monthly subscription. If you’d like to give it a try, go to www.realitydefender.ai, click “Request Trial Access” and mention HN in the comments field.<p>We’re here to answer your questions and hear your ideas, and would love to discuss any interesting use cases. We’d also be thrilled to collaborate with anyone who wants to integrate our API or who is working, or would like to work, in this space. We look forward to your comments and conversation!",
  "keywords": [],
  "author": {
    "@type": "Person",
    "name": "bpcrd",
    "url": "https://news.ycombinator.com/user?id=bpcrd"
  },
  "discussionUrl": "https://news.ycombinator.com/item?id=30766050",
  "sameAs": "https://news.ycombinator.com/item?id=30766050",
  "dateCreated": "2022-03-22T14:10:09.643Z",
  "datePublished": "2022-03-22T13:49:26.000Z",
  "dateModified": "2022-03-22T14:10:09.643Z",
  "interactionStatistic": [
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "LikeAction"
      },
      "userInteractionCount": 11
    },
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "CommentAction"
      },
      "userInteractionCount": 0
    }
  ],
  "headline_zh-Hans": "启动HN：现实卫士（YC W22）--深度虚假检测平台",
  "headline_zh-Hant": "啟動HN：現實衛士（YC W22）--深度虛假檢測平臺",
  "@context": [
    "https://schema.org",
    {
      "@vocab": "http://schema.org/",
      "@language": "en",
      "headline_zh-Hans": {
        "@id": "headline",
        "@language": "zh-Hans"
      },
      "headline_zh-Hant": {
        "@id": "headline",
        "@language": "zh-Hant"
      },
      "@version": 1.1,
      "description_zh-Hans": {
        "@id": "description",
        "@language": "zh-Hans"
      },
      "description_zh-Hant": {
        "@id": "description",
        "@language": "zh-Hant"
      }
    }
  ],
  "description_zh-Hans": "嗨，HN，我们是现实卫士（<a href=\"https://www.realitydefender.ai\" rel=\"nofollow\">https://www.realitydefender.ai</a>）的Ben、Gaurav和Ali。我们帮助公司、政府和记者确定媒体是真的还是假的，重点是音频、视频和图像的操纵。我们的API和网络应用程序提供实时扫描、风险评分和PDF报告卡。<p>机器学习的最新进展使我们有可能创造出真实人物的图像、视频和音频，说他们从未说过或做过的事情。最近，这项技术的普及使任何人都能创造出高度逼真的深度伪造品。虽然有些深度伪造是有经验的观察者仔细观察就能发现的，但许多人要么没有经验，要么不总是仔细观察--当然，这项技术只是在继续改进。这标志着不良行为者歪曲现实的能力有了飞跃，危及金融交易、个人和品牌声誉、公众舆论，甚至国家安全。<p>我们是一个拥有哈佛大学、纽约大学和加州大学洛杉矶分校数据科学博士和硕士学位的团队。在我们之间，我们在高盛、谷歌、中情局、联邦存款保险公司、国防部和哈佛大学应用研究部门拥有数十年的机器学习和网络安全交叉的经验。但我们目前的工作始于一个相当不可能的项目：我们试图复制迪帕克-乔普拉。我们与他合作，建立一个逼真的深层假象，使用户能够从他们的iPhone上与 \"数字迪帕克 \"进行实时对话。创建Deepak deepfake出乎意料地简单，其结果是如此惊人地逼真，以至于我们立即开始寻找能够帮助用户区分合成版本和真实版本的模型。<p>我们没有找到一个可靠的解决方案。我们感到沮丧，因为我们已经花了一个星期的时间在我们认为会占用我们咖啡时间的事情上，我们加倍努力，着手建立我们自己的模型，以检测被操纵的媒体。<p>经过调查，我们了解到为什么不存在一个持续准确的解决方案。各个公司（包括Facebook和微软）都试图建立自己的银弹、单一模型检测方法，或者如我们所称，用一个模型来统治它们。出于同样的原因，永远不会有一个单一的模型来解决反病毒、恶意软件等问题。<p>我们认为，对这个问题的任何认真的解决方案都需要一个 \"多模型 \"的方法，将最好的深度造假检测算法整合到一个聚合的模型中。 <p>我们挑战自己，建立一个可扩展的解决方案，将我们最好的深度造假检测模型与我们的合作者（微软、加州大学伯克利分校、哈佛大学）的模型整合起来。我们从一个网络应用程序的概念验证开始，很快就收到了来自政府、公司和研究人员的数百份访问请求。<p>我们的第一批用户在一些深度造假的情况下转向我们的平台，这些情况从糟糕到彻底的可怕。俄罗斯针对乌克兰和西方的虚假信息；模仿银行高管要求电汇的音频；马来西亚政府领导层的丑闻视频；参与者让自己看起来更年轻的色情作品；带有人工智能生成的专业照片的约会资料。不用说，所有这些都是完全虚假的！<p>就像计算机病毒一样，深度伪造将继续发展以规避当前的安全措施。新的深度假货检测技术必须像生成方法一样不断迭代。我们的解决方案不仅接受这一点，而且还拥抱它。我们迅速加入、测试和调整第三方模型，将其整合到我们的模型栈中，然后可以通过我们的网络应用程序和API访问这些模型。我们的任务已经吸引了几十个研究人员，他们贡献了他们的工作进行测试和调整，我们已经想出了一个有趣的合作商业模式：当他们的模型达到我们的基线分数时，只要他们继续在我们的平台上表现，我们就提供收入分成。(如果你有兴趣参与，我们很乐意听到你的意见！）<p>我们继续扩大我们的网络应用，并推出了一个API，我们正在向试点客户推广。目前，最受欢迎的用例是。KYC入职欺诈检测和语音识别。",
  "description_zh-Hant": "嗨，HN，我們是現實衛士（<a href=\"https://www.realitydefender.ai\" rel=\"nofollow\">https://www.realitydefender.ai</a>）的Ben、Gaurav和Ali。我們幫助公司、政府和記者確定媒體是真的還是假的，重點是音頻、視頻和圖像的操縱。我們的API和網絡應用程序提供實時掃描、風險評分和PDF報告卡。<p>機器學習的最新進展使我們有可能創造出真實人物的圖像、視頻和音頻，說他們從未說過或做過的事情。最近，這項技術的普及使任何人都能創造出高度逼真的深度偽造品。雖然有些深度偽造是有經驗的觀察者仔細觀察就能發現的，但許多人要麼沒有經驗，要麼不總是仔細觀察--當然，這項技術只是在繼續改進。這標誌著不良行為者歪曲現實的能力有了飛躍，危及金融交易、個人和品牌聲譽、公眾輿論，甚至國家安全。<p>我們是一個擁有哈佛大學、紐約大學和加州大學洛杉磯分校數據科學博士和碩士學位的團隊。在我們之間，我們在高盛、谷歌、中情局、聯邦存款保險公司、國防部和哈佛大學應用研究部門擁有數十年的機器學習和網絡安全交叉的經驗。但我們目前的工作始於一個相當不可能的項目：我們試圖複製迪帕克-喬普拉。我們與他合作，建立一個逼真的深層假象，使用戶能夠從他們的iPhone上與 \"數字迪帕克 \"進行實時對話。創建Deepak deepfake出乎意料地簡單，其結果是如此驚人地逼真，以至於我們立即開始尋找能夠幫助用戶區分合成版本和真實版本的模型。<p>我們沒有找到一個可靠的解決方案。我們感到沮喪，因為我們已經花了一個星期的時間在我們認為會佔用我們咖啡時間的事情上，我們加倍努力，著手建立我們自己的模型，以檢測被操縱的媒體。<p>經過調查，我們瞭解到為什麼不存在一個持續準確的解決方案。各個公司（包括Facebook和微軟）都試圖建立自己的銀彈、單一模型檢測方法，或者如我們所稱，用一個模型來統治它們。出於同樣的原因，永遠不會有一個單一的模型來解決反病毒、惡意軟件等問題。<p>我們認為，對這個問題的任何認真的解決方案都需要一個 \"多模型 \"的方法，將最好的深度造假檢測算法整合到一個聚合的模型中。 <p>我們挑戰自己，建立一個可擴展的解決方案，將我們最好的深度造假檢測模型與我們的合作者（微軟、加州大學伯克利分校、哈佛大學）的模型整合起來。我們從一個網絡應用程序的概念驗證開始，很快就收到了來自政府、公司和研究人員的數百份訪問請求。<p>我們的第一批用戶在一些深度造假的情況下轉向我們的平臺，這些情況從糟糕到徹底的可怕。俄羅斯針對烏克蘭和西方的虛假信息；模仿銀行高管要求電匯的音頻；馬來西亞政府領導層的醜聞視頻；參與者讓自己看起來更年輕的色情作品；帶有人工智能生成的專業照片的約會資料。不用說，所有這些都是完全虛假的！<p>就像計算機病毒一樣，深度偽造將繼續發展以規避當前的安全措施。新的深度假貨檢測技術必須像生成方法一樣不斷迭代。我們的解決方案不僅接受這一點，而且還擁抱它。我們迅速加入、測試和調整第三方模型，將其整合到我們的模型棧中，然後可以通過我們的網絡應用程序和API訪問這些模型。我們的任務已經吸引了幾十個研究人員，他們貢獻了他們的工作進行測試和調整，我們已經想出了一個有趣的合作商業模式：當他們的模型達到我們的基線分數時，只要他們繼續在我們的平臺上表現，我們就提供收入分成。(如果你有興趣參與，我們很樂意聽到你的意見！）<p>我們繼續擴大我們的網絡應用，並推出了一個API，我們正在向試點客戶推廣。目前，最受歡迎的用例是。KYC入職欺詐檢測和語音識別。"
}