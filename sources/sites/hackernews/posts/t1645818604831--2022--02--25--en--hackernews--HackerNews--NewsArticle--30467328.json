{
  "@type": "NewsArticle",
  "identifier": "2022--02--25--en--hackernews--HackerNews--NewsArticle--30467328",
  "url": "https://news.ycombinator.com/item?id=30467328",
  "headline": "Show HN: Cloning a musical instrument from 16 seconds of audio",
  "publisher": {
    "@type": "Organization",
    "name": "HackerNews",
    "url": "https://news.ycombinator.com",
    "logo": "https://hn.buzzing.cc/avatar.png"
  },
  "description": "In 2020, Magenta released DDSP [1], a machine learning algorithm / python library which made it possible to generate good sounding instrument synthesizers from about 6-10 minutes of data. While working with DDSP for a project, we realised how \nit was actually quite hard to find 6-10 minute of clean recordings of monophonic instruments.<p>In this project, we have combined the DDSP  architecture with a domain adaptation technique from speech synthesis [2]. This domain adaptation technique works by pre-training our model on many different recordings from the Solos dataset [3] first and then fine-tuning parts of the model to the new recording. This allows us to produce decent sounding instrument synthesisers from as little as 16 seconds of target audio instead of 6-10 minutes.<p>[1] <a href=\"https://arxiv.org/abs/2001.04643\" rel=\"nofollow\">https://arxiv.org/abs/2001.04643</a><p>[2] <a href=\"https://arxiv.org/abs/1802.06006\" rel=\"nofollow\">https://arxiv.org/abs/1802.06006</a><p>[3] <a href=\"https://arxiv.org/abs/2006.07931\" rel=\"nofollow\">https://arxiv.org/abs/2006.07931</a><p>We hope to publish a paper on the topic soon.",
  "keywords": [
    "Show HN"
  ],
  "genre": "Show HN",
  "author": {
    "@type": "Person",
    "name": "abdljasser2",
    "url": "https://news.ycombinator.com/user?id=abdljasser2"
  },
  "discussionUrl": "https://news.ycombinator.com/item?id=30467328",
  "sameAs": "https://erlj.notion.site/Neural-Instrument-Cloning-from-very-few-samples-2cf41d8b630842ee8c7eb55036a1bfd6",
  "dateCreated": "2022-02-25T19:50:04.831Z",
  "datePublished": "2022-02-25T13:59:59.000Z",
  "dateModified": "2022-02-25T19:50:04.831Z",
  "interactionStatistic": [
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "LikeAction"
      },
      "userInteractionCount": 18
    },
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "CommentAction"
      },
      "userInteractionCount": 1
    }
  ],
  "headline_zh-Hans": "Show HN: 从16秒的音频中克隆出一种乐器",
  "headline_zh-Hant": "Show HN: 從16秒的音頻中克隆出一種樂器",
  "@context": [
    "https://schema.org",
    {
      "@vocab": "http://schema.org/",
      "@language": "en",
      "headline_zh-Hans": {
        "@id": "headline",
        "@language": "zh-Hans"
      },
      "headline_zh-Hant": {
        "@id": "headline",
        "@language": "zh-Hant"
      },
      "@version": 1.1,
      "description_zh-Hans": {
        "@id": "description",
        "@language": "zh-Hans"
      },
      "description_zh-Hant": {
        "@id": "description",
        "@language": "zh-Hant"
      }
    }
  ],
  "description_zh-Hans": "2020年，Magenta发布了DDSP[1]，这是一个机器学习算法/python库，使得从大约6-10分钟的数据中产生好听的乐器合成器成为可能。在为一个项目使用DDSP时，我们意识到 \n<p>在这个项目中，我们将DDSP架构与语音合成的领域适应技术相结合[2]。这种领域适应技术的工作原理是，首先在Solos数据集[3]的许多不同录音上对我们的模型进行预训练，然后根据新的录音对模型的一部分进行微调。This allows us to produce decent sounding instrument synthesisers from as little as 16 seconds of target audio instead of 6-10 minutes.<p>[1] <a href=\"https://arxiv.org/abs/2001.04643\" rel=\"nofollow\">https://arxiv.org/abs/2001.04643</a><p>[2] <a href=\"https://arxiv.org/abs/1802.06006\" rel=\"nofollow\">https://arxiv.org/abs/1802.06006</a><p>[3] <a href=\"https://arxiv.org/abs/2006.07931\" rel=\"nofollow\">https://arxiv.org/abs/2006.07931</a><p>We hope to publish a paper on the topic soon.",
  "description_zh-Hant": "2020年，Magenta發佈了DDSP[1]，這是一個機器學習算法/python庫，使得從大約6-10分鐘的數據中產生好聽的樂器合成器成為可能。在為一個項目使用DDSP時，我們意識到 \n<p>在這個項目中，我們將DDSP架構與語音合成的領域適應技術相結合[2]。這種領域適應技術的工作原理是，首先在Solos數據集[3]的許多不同錄音上對我們的模型進行預訓練，然後根據新的錄音對模型的一部分進行微調。This allows us to produce decent sounding instrument synthesisers from as little as 16 seconds of target audio instead of 6-10 minutes.<p>[1] <a href=\"https://arxiv.org/abs/2001.04643\" rel=\"nofollow\">https://arxiv.org/abs/2001.04643</a><p>[2] <a href=\"https://arxiv.org/abs/1802.06006\" rel=\"nofollow\">https://arxiv.org/abs/1802.06006</a><p>[3] <a href=\"https://arxiv.org/abs/2006.07931\" rel=\"nofollow\">https://arxiv.org/abs/2006.07931</a><p>We hope to publish a paper on the topic soon."
}