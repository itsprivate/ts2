{
  "@type": "NewsArticle",
  "identifier": "2022--06--22--en--hackernews--HackerNews--NewsArticle--31837307",
  "url": "https://news.ycombinator.com/item?id=31837307",
  "headline": "Show HN: Data Diff – compare tables of any size across databases",
  "publisher": {
    "@type": "Organization",
    "name": "HackerNews",
    "url": "https://news.ycombinator.com",
    "logo": "https://hn.buzzing.cc/avatar.png"
  },
  "description": "Gleb, Alex, Erez and Simon here – we are building an open-source tool for comparing data within and across databases at any scale. The repo is at <a href=\"https://github.com/datafold/data-diff\" rel=\"nofollow\">https://github.com/datafold/data-diff</a>, and our home page is <a href=\"https://datafold.com/\" rel=\"nofollow\">https://datafold.com/</a>.<p>As a company, Datafold builds tools for data engineers to automate the most tedious and error-prone tasks falling through the cracks of the modern data stack, such as data testing and lineage. We launched two years ago with a tool for regression-testing changes to ETL code <a href=\"https://news.ycombinator.com/item?id=24071955\" rel=\"nofollow\">https://news.ycombinator.com/item?id=24071955</a>. It compares the produced data before and after the code change and shows the impact on values, aggregate metrics, and downstream data applications.<p>While working with many customers on improving their data engineering experience, we kept hearing that they needed to diff their data across databases to validate data replication between systems.<p>There were 3 main use cases for such replication:<p>(1) To perform analytics on transactional data in an OLAP engine (e.g. PostgreSQL &gt; Snowflake)\n(2) To migrate between transactional stores (e.g. MySQL &gt; PostgreSQL)\n(3) To leverage data in a specialized engine (e.g. PostgreSQL &gt; ElasticSearch).<p>Despite multiple vendors (e.g., Fivetran, Stitch) and open-source products (Airbyte, Debezium) solving data replication, there was no tooling for validating the correctness of such replication. When we researched how teams were going about this, we found that most have been either:<p>Running manual checks: e.g., starting with COUNT(*) and then digging into the discrepancies, which often took hours to pinpoint the inconsistencies.\nUsing distributed MPP engines such as Spark or Trino to download the complete datasets from both databases and then comparing them in memory – an expensive process requiring complex infrastructure.<p>Our users wanted a tool that could:<p>(1) Compare datasets quickly (seconds/minutes) at a large (millions/billions of rows) scale across different databases (2) Have minimal network IO and database workload overhead. (3) Provide straightforward output: basic stats and what rows are different. (4) Be embedded into a data orchestrator such as Airflow to run right after the replication process.<p>So we built Data Diff as an open-source package available through pip. Data Diff can be run in a CLI or wrapped into any data orchestrator such as Airflow, Dagster, etc.<p>To solve for speed at scale with minimal overhead, Data Diff relies on checksumming the data in both databases and uses binary search to identify diverging records. That way, it can compare arbitrarily large datasets in logarithmic time and IO – only transferring a tiny fraction of the data over the network. For example, it can diff tables with 25M rows in ~10s and 1B+ rows in ~5m across two physically separate PostgreSQL databases while running on a typical laptop.<p>We've launched this tool under the MIT license so that any developer can use it, and to encourage contributions of other database connectors. We didn't want to charge engineers for such a fundamental use case. We make money by charging a license fee for advanced solutions such as column-level data lineage, CI workflow automation, and ML-powered alerts.",
  "keywords": [
    "Show HN"
  ],
  "genre": "Show HN",
  "author": {
    "@type": "Person",
    "name": "hichkaker",
    "url": "https://news.ycombinator.com/user?id=hichkaker"
  },
  "discussionUrl": "https://news.ycombinator.com/item?id=31837307",
  "sameAs": "https://news.ycombinator.com/item?id=31837307",
  "dateCreated": "2022-06-22T19:56:15.358Z",
  "datePublished": "2022-06-22T15:28:04.000Z",
  "dateModified": "2022-06-22T19:56:15.358Z",
  "interactionStatistic": [
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "LikeAction"
      },
      "userInteractionCount": 29
    },
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "CommentAction"
      },
      "userInteractionCount": 0
    }
  ],
  "headline_zh-Hans": "Show HN: 数据差异--跨数据库比较任何大小的表\n",
  "headline_zh-Hant": "Show HN: 數據差異--跨數據庫比較任何大小的表\n",
  "@context": [
    "https://schema.org",
    {
      "@vocab": "http://schema.org/",
      "@language": "en",
      "headline_zh-Hans": {
        "@id": "headline",
        "@language": "zh-Hans"
      },
      "headline_zh-Hant": {
        "@id": "headline",
        "@language": "zh-Hant"
      },
      "@version": 1.1,
      "description_zh-Hans": {
        "@id": "description",
        "@language": "zh-Hans"
      },
      "description_zh-Hant": {
        "@id": "description",
        "@language": "zh-Hant"
      }
    }
  ],
  "description_zh-Hans": "Gleb, Alex, Erez and Simon here – we are building an open-source tool for comparing data within and across databases at any scale. The repo is at <a href=\"https://github.com/datafold/data-diff\" rel=\"nofollow\">https://github.com/datafold/data-diff</a>, and our home page is <a href=\"https://datafold.com/\" rel=\"nofollow\">https://datafold.com/</a>.<p>As a company, Datafold builds tools for data engineers to automate the most tedious and error-prone tasks falling through the cracks of the modern data stack, such as data testing and lineage. We launched two years ago with a tool for regression-testing changes to ETL code <a href=\"https://news.ycombinator.com/item?id=24071955\" rel=\"nofollow\">https://news.ycombinator.com/item?id=24071955</a>. It compares the produced data before and after the code change and shows the impact on values, aggregate metrics, and downstream data applications.<p>While working with many customers on improving their data engineering experience, we kept hearing that they needed to diff their data across databases to validate data replication between systems.<p>There were 3 main use cases for such replication:<p>(1) To perform analytics on transactional data in an OLAP engine (e.g. PostgreSQL &gt; Snowflake)\n(2) To migrate between transactional stores (e.g. MySQL &gt; PostgreSQL)\n(3) To leverage data in a specialized engine (e.g. PostgreSQL &gt; ElasticSearch).<p>Despite multiple vendors (e.g., Fivetran, Stitch) and open-source products (Airbyte, Debezium) solving data replication, there was no tooling for validating the correctness of such replication. When we researched how teams were going about this, we found that most have been either:<p>Running manual checks: e.g., starting with COUNT(*) and then digging into the discrepancies, which often took hours to pinpoint the inconsistencies.\nUsing distributed MPP engines such as Spark or Trino to download the complete datasets from both databases and then comparing them in memory – an expensive process requiring complex infrastructure.<p>Our users wanted a tool that could:<p>(1) Compare datasets quickly (seconds/minutes) at a large (millions/billions of rows) scale across different databases (2) Have minimal network IO and database workload overhead. (3) 提供直接的输出：基本统计数字和哪些行是不同的。(4) 嵌入到数据协调器中，如Airflow，以便在复制过程后立即运行。<p>因此，我们将Data Diff作为一个开源包，通过pip提供。Data Diff可以在CLI中运行，也可以封装到任何数据协调器中，如Airflow、Dagster等。<p>为了以最小的开销解决大规模的速度问题，Data Diff依赖于对两个数据库的数据进行校验，并使用二进制搜索来识别不同的记录。这样，它可以在对数时间和IO中比较任意大的数据集--只在网络上传输极小部分的数据。例如，它可以在一个典型的笔记本电脑上运行时，在两个物理上独立的PostgreSQL数据库中，在~10s内对有2500万行的表进行比较，在~5m内对有1B+行的表进行比较。<p>我们在MIT许可下推出这个工具，以便任何开发者可以使用它，并鼓励其他数据库连接器的贡献。我们不想为这样一个基本的用例向工程师收费。我们通过对高级解决方案收取许可费来赚钱，如列级数据线、CI工作流自动化和ML驱动的警报。\n",
  "description_zh-Hant": "Gleb, Alex, Erez and Simon here – we are building an open-source tool for comparing data within and across databases at any scale. The repo is at <a href=\"https://github.com/datafold/data-diff\" rel=\"nofollow\">https://github.com/datafold/data-diff</a>, and our home page is <a href=\"https://datafold.com/\" rel=\"nofollow\">https://datafold.com/</a>.<p>As a company, Datafold builds tools for data engineers to automate the most tedious and error-prone tasks falling through the cracks of the modern data stack, such as data testing and lineage. We launched two years ago with a tool for regression-testing changes to ETL code <a href=\"https://news.ycombinator.com/item?id=24071955\" rel=\"nofollow\">https://news.ycombinator.com/item?id=24071955</a>. It compares the produced data before and after the code change and shows the impact on values, aggregate metrics, and downstream data applications.<p>While working with many customers on improving their data engineering experience, we kept hearing that they needed to diff their data across databases to validate data replication between systems.<p>There were 3 main use cases for such replication:<p>(1) To perform analytics on transactional data in an OLAP engine (e.g. PostgreSQL &gt; Snowflake)\n(2) To migrate between transactional stores (e.g. MySQL &gt; PostgreSQL)\n(3) To leverage data in a specialized engine (e.g. PostgreSQL &gt; ElasticSearch).<p>Despite multiple vendors (e.g., Fivetran, Stitch) and open-source products (Airbyte, Debezium) solving data replication, there was no tooling for validating the correctness of such replication. When we researched how teams were going about this, we found that most have been either:<p>Running manual checks: e.g., starting with COUNT(*) and then digging into the discrepancies, which often took hours to pinpoint the inconsistencies.\nUsing distributed MPP engines such as Spark or Trino to download the complete datasets from both databases and then comparing them in memory – an expensive process requiring complex infrastructure.<p>Our users wanted a tool that could:<p>(1) Compare datasets quickly (seconds/minutes) at a large (millions/billions of rows) scale across different databases (2) Have minimal network IO and database workload overhead. (3) 提供直接的輸出：基本統計數字和哪些行是不同的。(4) 嵌入到數據協調器中，如Airflow，以便在複製過程後立即運行。<p>因此，我們將Data Diff作為一個開源包，通過pip提供。Data Diff可以在CLI中運行，也可以封裝到任何數據協調器中，如Airflow、Dagster等。<p>為了以最小的開銷解決大規模的速度問題，Data Diff依賴於對兩個數據庫的數據進行校驗，並使用二進制搜索來識別不同的記錄。這樣，它可以在對數時間和IO中比較任意大的數據集--只在網絡上傳輸極小部分的數據。例如，它可以在一個典型的筆記本電腦上運行時，在兩個物理上獨立的PostgreSQL數據庫中，在~10s內對有2500萬行的表進行比較，在~5m內對有1B+行的表進行比較。<p>我們在MIT許可下推出這個工具，以便任何開發者可以使用它，並鼓勵其他數據庫連接器的貢獻。我們不想為這樣一個基本的用例向工程師收費。我們通過對高級解決方案收取許可費來賺錢，如列級數據線、CI工作流自動化和ML驅動的警報。\n"
}