{
  "@type": "NewsArticle",
  "identifier": "2022--03--11--en--hackernews--HackerNews--NewsArticle--30643520",
  "url": "https://news.ycombinator.com/item?id=30643520",
  "headline": "Launch HN: Tensil (YC S19) – Open-Source ML Accelerators",
  "publisher": {
    "@type": "Organization",
    "name": "HackerNews",
    "url": "https://news.ycombinator.com",
    "logo": "https://hn.buzzing.cc/avatar.png"
  },
  "description": "Hello HN! I'm Tom, co-founder at Tensil (<a href=\"https://www.tensil.ai/\" rel=\"nofollow\">https://www.tensil.ai/</a>). We design free and open source machine learning accelerators that anyone can use.<p>A machine learning inference accelerator is a specialized chip that can run the operations used in ML models very quickly and efficiently. It can be either an ASIC or an FPGA, with ASIC giving better performance but FPGA being more flexible.<p>Custom accelerators offer dramatically better performance per watt than existing GPU and CPU options. Massive companies like Google and Facebook use them to make training and inference cheaper. However, everyone else has been left out: small and mid-sized companies, students and academics, hobbyists and tinkerers currently have no chance of getting ML hardware that perfectly suits their needs. We aim to change that, starting with ML inference on embedded and edge FPGA platforms. Our dream is that our accelerators help people make new applications possible that simply weren't feasible before.<p>We believe that advances in AI go hand in hand with advances in computing hardware. As a couple of software and ML engineers hoping to live in a world alongside intelligent machines, we wanted to know why those hardware advances were taking so long! We taught ourselves digital design and gradually realized that the next generation of hardware will need to be finely customized to enable state of the art ML models at the edge, that is, running on your devices and not in the cloud. In the CPU world, the RISC-V RocketChip implementation has proven the value of customizable compute hardware. The problem was that no-one was building that kind of capability for ML acceleration. We started Tensil to build customizable ML accelerators and see what kind of applications people can create with them.<p>Tensil is a set of tools for running ML models on custom accelerator architectures. It includes an RTL generator, a model compiler, and a set of drivers. It enables you to create a custom accelerator, compile an ML model targeted at it, and then deploy and run that compiled model. To see how to do this and get it running on an FPGA platform, check out our tutorial at <a href=\"https://www.tensil.ai/docs/tutorials/resnet20-ultra96v2/\" rel=\"nofollow\">https://www.tensil.ai/docs/tutorials/resnet20-ultra96v2/</a>.<p>We developed an accelerator generator in Chisel and then wrote a parameterizable graph compiler in Scala. (Fun fact: unlike in software, formal verification is actually a totally viable way to test digital circuits and we have made great use of this technique.) The accelerator generator takes in the desired architecture parameters and produces an instance of the accelerator which can be synthesized using standard EDA tools. The compiler implements ML models using the accelerator’s instruction set and can target any possible instance of the accelerator.<p>Currently, the accelerator architecture is based around a systolic array, similar to well-known ML ASICs. You can view the architecture spec in our documentation. The compiler performs a wide variety of tasks but is optimized for convolutional neural networks. There are also drivers for each supported platform, currently limited to FPGAs running bare-metal or with a host OS.<p>When you tell the driver to run your ML model, it sets up the input data and then streams the compiled model into the accelerator. The accelerator independently accesses host memory during execution. When the accelerator is done, the driver is notified and looks for the output in the pre-assigned area of host memory.<p>How are we different from other accelerator options? There are many ML ASICs out there but they are all locked into a single architecture, whereas we have customization at the core of our technology. This offers the potential for a better trade-off between performance/price/watts/accuracy. Compared with other FPGA options, Xilinx DPU is great but it’s closed source and can be difficult to work with if your model is in any way customized. By going open source, we aim to support the widest possible range of models. FINN is a very cool project but requires big changes to your model in order to work, and also typically requires large FPGAs which are unsuitable for edge deployments. We work out of the box with any model (no need to quantize), and on small edge FPGAs. For embedded systems, tflite/tfmicro are great for deploying very small ML models on extremely constrained edge devices, but they are limited in terms of the performance and accuracy that can be achieved. Our tools allow you to work with full size state of the art models at high accuracy and speed.<p>Currently we're focused on the edge and embedded ML inference use case. If you\nrun ML models using any of the major frameworks (TensorFlow/Keras, PyTorch, etc.) on small, embedded or edge devices then Tensil is a good fit for you right now. If you primarily run inference in the data center or need lots of training acceleration, reach out to us and we can walk you through our roadmap. For now we are focused on CNN inference on edge FPGA platforms, but our aim is to support all model architectures on a wide variety of fabrics for both training and inference.<p>The core technology will always be free and open source, but we plan to offer a “pro” version with extra enterprise features under a dual license arrangement, similar to Gitlab. We are also working on a cloud service for running our tools in a hosted setup, in which you’ll be able to run a search across all possible Tensil architectures to automatically find the best FPGA for your model.<p>If you're interested to learn more, check out our docs (<a href=\"https://www.tensil.ai/docs\" rel=\"nofollow\">https://www.tensil.ai/docs</a>), our Github repo (<a href=\"https://github.com/tensil-ai/tensil\" rel=\"nofollow\">https://github.com/tensil-ai/tensil</a>) and join our Discord (<a href=\"https://discord.gg/TSw34H3PXr\" rel=\"nofollow\">https://discord.gg/TSw34H3PXr</a>). And feel free to reach out any time (email in profile).<p>We’re here to enable you to develop amazing new ML based applications, so we’d love to hear your experiences of working with ML compute hardware, whether it be CPU, GPU, or some other specialized platform. Have you had to make major changes to your ML models to get them to run on the available hardware? Are there any cool features or UX improvements that you wish hardware makers would add? Are there features that you’d like to add to your own applications but don’t know how you’d get them to work on an edge device? Looking forward to your comments!",
  "keywords": [],
  "author": {
    "@type": "Person",
    "name": "tdba",
    "url": "https://news.ycombinator.com/user?id=tdba"
  },
  "discussionUrl": "https://news.ycombinator.com/item?id=30643520",
  "sameAs": "https://www.tensil.ai/#",
  "dateCreated": "2022-03-11T18:11:49.623Z",
  "datePublished": "2022-03-11T18:00:10.000Z",
  "dateModified": "2022-03-11T18:11:49.623Z",
  "interactionStatistic": [
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "LikeAction"
      },
      "userInteractionCount": 5
    },
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "CommentAction"
      },
      "userInteractionCount": 0
    }
  ],
  "headline_zh-Hans": "启动HN：Tensil (YC S19) - 开源的ML加速器",
  "headline_zh-Hant": "啟動HN：Tensil (YC S19) - 開源的ML加速器",
  "@context": [
    "https://schema.org",
    {
      "@vocab": "http://schema.org/",
      "@language": "en",
      "headline_zh-Hans": {
        "@id": "headline",
        "@language": "zh-Hans"
      },
      "headline_zh-Hant": {
        "@id": "headline",
        "@language": "zh-Hant"
      },
      "@version": 1.1,
      "description_zh-Hans": {
        "@id": "description",
        "@language": "zh-Hans"
      },
      "description_zh-Hant": {
        "@id": "description",
        "@language": "zh-Hant"
      }
    }
  ],
  "description_zh-Hans": "你好，HN! 我是汤姆，Tensil（<a href=\"https://www.tensil.ai/\" rel=\"nofollow\">https://www.tensil.ai/</a>）的联合创始人。我们设计了任何人都可以使用的免费和开源的机器学习加速器。<p>机器学习推理加速器是一种专门的芯片，可以非常快速和有效地运行ML模型中的操作。它既可以是ASIC，也可以是FPGA，ASIC的性能更好，而FPGA则更灵活。像谷歌和Facebook这样的大型公司使用它们来使训练和推理更加便宜。然而，其他所有人都被排除在外：中小型公司、学生和学者、业余爱好者和修理工目前没有机会获得完全适合他们需求的ML硬件。我们的目标是改变这种状况，从嵌入式和边缘FPGA平台的ML推理开始。我们的梦想是，我们的加速器能够帮助人们实现以前根本不可能实现的新应用。<p>我们相信，人工智能的进步与计算硬件的进步是相辅相成的。作为一对软件和ML工程师，我们希望生活在一个与智能机器并存的世界里，我们想知道为什么这些硬件的进步需要这么长的时间！我们自学了数字设计，并逐渐意识到这一点。我们自学了数字设计，并逐渐意识到，下一代硬件将需要精细的定制，以便在边缘实现最先进的ML模型，也就是说，在你的设备上而不是在云端运行。在CPU领域，RISC-V RocketChip的实现已经证明了可定制计算硬件的价值。问题是，没有人在为ML加速建立这种能力。我们启动了Tensil，以建立可定制的ML加速器，并看看人们可以用它们创造什么样的应用。<p>Tensil是一套在定制加速器架构上运行ML模型的工具。它包括一个RTL生成器、一个模型编译器和一套驱动程序。它使你能够创建一个自定义加速器，编译一个针对它的ML模型，然后部署和运行该编译模型。要了解如何做到这一点并使其在FPGA平台上运行，请查看我们的教程：<a href=\"https://www.tensil.ai/docs/tutorials/resnet20-ultra96v2/\" rel=\"nofollow\">https://www.tensil.ai/docs/tutorials/resnet20-ultra96v2/</a>。<p>我们在Chisel中开发了一个加速器生成器，然后在Scala中编写了一个可参数化的图编译器。(有趣的是：与软件不同，形式化验证实际上是测试数字电路的一种完全可行的方法，我们已经很好地利用了这种技术)。加速器生成器接收所需的架构参数，并产生一个加速器的实例，可以使用标准的EDA工具进行合成。编译器使用加速器的指令集实现ML模型，并可以针对加速器的任何可能的实例。<p>目前，加速器架构是基于一个收缩阵列，与著名的ML ASIC类似。你可以在我们的文档中查看该架构规格。编译器可以执行各种各样的任务，但对卷积神经网络进行了优化。每个支持的平台都有驱动程序，目前仅限于运行裸机或主机操作系统的FPGA。<p>当你告诉驱动程序运行你的ML模型时，它设置了输入数据，然后将编译后的模型流入加速器。加速器在执行期间独立访问主机内存。当加速器完成时，驱动器被通知并在主机内存的预分配区域寻找输出。<p>我们与其他加速器选项有何不同？目前有许多ML ASIC，但它们都被锁定在一个单一的架构中，而我们的技术核心是定制。这为在性能/价格/瓦特/精度之间进行更好的权衡提供了可能。与其他FPGA选择相比，赛灵思DPU很好，但它是闭源的，如果你的模型是以任何方式定制的，就很难使用。通过开源，我们旨在支持尽可能多的模型。FINN是一个非常酷的项目，但是需要对你的模型做很大的改动才能工作，而且通常需要大型FPGA，不适合边缘部署。我们开箱即用，适用于任何模型（不需要量化），并且适用于小型边缘FPGA。对于嵌入式系统，tflite/tfmicro非常适用于在极其复杂的环境中部署非常小的ML模型。",
  "description_zh-Hant": "你好，HN! 我是湯姆，Tensil（<a href=\"https://www.tensil.ai/\" rel=\"nofollow\">https://www.tensil.ai/</a>）的聯合創始人。我們設計了任何人都可以使用的免費和開源的機器學習加速器。<p>機器學習推理加速器是一種專門的芯片，可以非常快速和有效地運行ML模型中的操作。它既可以是ASIC，也可以是FPGA，ASIC的性能更好，而FPGA則更靈活。像谷歌和Facebook這樣的大型公司使用它們來使訓練和推理更加便宜。然而，其他所有人都被排除在外：中小型公司、學生和學者、業餘愛好者和修理工目前沒有機會獲得完全適合他們需求的ML硬件。我們的目標是改變這種狀況，從嵌入式和邊緣FPGA平臺的ML推理開始。我們的夢想是，我們的加速器能夠幫助人們實現以前根本不可能實現的新應用。<p>我們相信，人工智能的進步與計算硬件的進步是相輔相成的。作為一對軟件和ML工程師，我們希望生活在一個與智能機器並存的世界裡，我們想知道為什麼這些硬件的進步需要這麼長的時間！我們自學了數字設計，並逐漸意識到這一點。我們自學了數字設計，並逐漸意識到，下一代硬件將需要精細的定製，以便在邊緣實現最先進的ML模型，也就是說，在你的設備上而不是在雲端運行。在CPU領域，RISC-V RocketChip的實現已經證明了可定製計算硬件的價值。問題是，沒有人在為ML加速建立這種能力。我們啟動了Tensil，以建立可定製的ML加速器，並看看人們可以用它們創造什麼樣的應用。<p>Tensil是一套在定製加速器架構上運行ML模型的工具。它包括一個RTL生成器、一個模型編譯器和一套驅動程序。它使你能夠創建一個自定義加速器，編譯一個針對它的ML模型，然後部署和運行該編譯模型。要了解如何做到這一點並使其在FPGA平臺上運行，請查看我們的教程：<a href=\"https://www.tensil.ai/docs/tutorials/resnet20-ultra96v2/\" rel=\"nofollow\">https://www.tensil.ai/docs/tutorials/resnet20-ultra96v2/</a>。<p>我們在Chisel中開發了一個加速器生成器，然後在Scala中編寫了一個可參數化的圖編譯器。(有趣的是：與軟件不同，形式化驗證實際上是測試數字電路的一種完全可行的方法，我們已經很好地利用了這種技術)。加速器生成器接收所需的架構參數，併產生一個加速器的實例，可以使用標準的EDA工具進行合成。編譯器使用加速器的指令集實現ML模型，並可以針對加速器的任何可能的實例。<p>目前，加速器架構是基於一個收縮陣列，與著名的ML ASIC類似。你可以在我們的文檔中查看該架構規格。編譯器可以執行各種各樣的任務，但對卷積神經網絡進行了優化。每個支持的平臺都有驅動程序，目前僅限於運行裸機或主機操作系統的FPGA。<p>當你告訴驅動程序運行你的ML模型時，它設置了輸入數據，然後將編譯後的模型流入加速器。加速器在執行期間獨立訪問主機內存。當加速器完成時，驅動器被通知並在主機內存的預分配區域尋找輸出。<p>我們與其他加速器選項有何不同？目前有許多ML ASIC，但它們都被鎖定在一個單一的架構中，而我們的技術核心是定製。這為在性能/價格/瓦特/精度之間進行更好的權衡提供了可能。與其他FPGA選擇相比，賽靈思DPU很好，但它是閉源的，如果你的模型是以任何方式定製的，就很難使用。通過開源，我們旨在支持儘可能多的模型。FINN是一個非常酷的項目，但是需要對你的模型做很大的改動才能工作，而且通常需要大型FPGA，不適合邊緣部署。我們開箱即用，適用於任何模型（不需要量化），並且適用於小型邊緣FPGA。對於嵌入式系統，tflite/tfmicro非常適用於在極其複雜的環境中部署非常小的ML模型。"
}