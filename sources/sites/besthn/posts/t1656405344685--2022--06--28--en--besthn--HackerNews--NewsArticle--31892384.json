{
  "@type": "NewsArticle",
  "identifier": "2022--06--28--en--besthn--HackerNews--NewsArticle--31892384",
  "url": "https://news.ycombinator.com/item?id=31892384",
  "headline": "Ask HN: What is your Kubernetes nightmare?",
  "publisher": {
    "@type": "Organization",
    "name": "HackerNews",
    "url": "https://news.ycombinator.com",
    "logo": "https://hn.buzzing.cc/avatar.png"
  },
  "description": "Everything self-hosted has its maintenance tax but why Kubernetes (especially self hosted) is so hard? What aspect is that makes Kubernetes operationally so hard?<p>- Is it the networking model that is simple from the consumption standpoint but has too many moving parts for it to be implemented?<p>- Is it the storage model, CSI and friends?<p>- Is it the bunch of controller loops doing their own things with nothing that gives a &quot;wholesome&quot; picture to identify the root cause?<p>For me personally, first and foremost thing on my mind is the networking details. They are &quot;automatically generated&quot; by each CNI solution in slightly different ways and constructs (iptables, virtual bridges, routing daemons, eBPF etc etc) and because they are generated, it is not uncommon to find hundreds of iptable rules and chains on a single node and/or similar configuration.<p>Being automated, these solutions generate tons of components/configurations which in case of trouble, even if one has mastery on them, would take some time to hoop through all the components (virtual interfaces, virtual bridges, iptable chains and rules, ipvs entries etc) to identify what's causing the trouble. Essentially, one pretty much has to be a network engineer because besides the underlying/physical (or the virtual, I mean cloud VPCs) network, k8s pulls its very own network (pod network, cluster network) implemented on the software/configuration layer which has to be fully understood to be able to maintained.<p>God forbid, if the CNI solution has some edge case or for some other misconfiguration, it keeps generating inadequate or misconfigured rules/routes etc resulting in a broken &quot;software defined network&quot; that I cannot identify in time on a production system is my nightmare and I don't know how to reduce that risk.<p>What's your Kubernetes nightmare?<p>EDIT: formating",
  "keywords": [
    "Ask HN"
  ],
  "genre": "Ask HN",
  "author": {
    "@type": "Person",
    "name": "wg0",
    "url": "https://news.ycombinator.com/user?id=wg0"
  },
  "discussionUrl": "https://news.ycombinator.com/item?id=31892384",
  "sameAs": "https://news.ycombinator.com/item?id=31892384",
  "dateCreated": "2022-06-28T08:35:44.685Z",
  "datePublished": "2022-06-27T09:39:57.000Z",
  "dateModified": "2022-06-28T08:35:44.685Z",
  "interactionStatistic": [
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "LikeAction"
      },
      "userInteractionCount": 200
    },
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "CommentAction"
      },
      "userInteractionCount": 236
    }
  ],
  "headline_zh-Hans": "Ask HN: 你的Kubernetes梦魇是什么？\n",
  "headline_zh-Hant": "Ask HN: 你的Kubernetes夢魘是什麼？\n",
  "@context": [
    "https://schema.org",
    {
      "@vocab": "http://schema.org/",
      "@language": "en",
      "headline_zh-Hans": {
        "@id": "headline",
        "@language": "zh-Hans"
      },
      "headline_zh-Hant": {
        "@id": "headline",
        "@language": "zh-Hant"
      },
      "@version": 1.1,
      "description_zh-Hans": {
        "@id": "description",
        "@language": "zh-Hans"
      },
      "description_zh-Hant": {
        "@id": "description",
        "@language": "zh-Hant"
      }
    }
  ],
  "description_zh-Hans": "所有自我托管的东西都有它的维护税，但为什么Kubernetes（尤其是自我托管）这么难？是什么让Kubernetes的运营如此艰难？<p>- 是网络模型，从消费的角度看很简单，但有太多的移动部件，无法实施？<p>- 是存储模型，CSI和朋友？<p>- 是一堆控制器循环做自己的事情，没有任何东西可以提供一个&quot;健康&quot;的图片来识别根本原因？<p>对我个人来说，我想到的首要事情是网络细节。它们是由每个 CNI 解决方案以略微不同的方式和结构（iptables、虚拟网桥、路由守护程序、eBPF 等等等）自动生成的，由于它们是生成的，所以在单个节点和/或类似配置上发现数百条iptable规则和链的情况并不罕见。 <p>由于是自动化的，这些解决方案产生了大量的组件/配置，在出现问题的情况下，即使一个人掌握了这些组件，也需要一些时间来通过所有的组件（虚拟接口、虚拟桥、iptable链和规则、ipvs条目等）来识别导致问题的原因。从本质上讲，一个人几乎必须是一个网络工程师，因为除了底层/物理（或虚拟，我指的是云VPC）网络，k8s在软件/配置层实现了自己的网络（pod网络，集群网络），必须完全理解才能维护。 <p>上帝保佑，如果CNI解决方案有一些边缘情况或其他错误配置，它不断产生不充分或错误配置的规则/路由等，导致一个破碎的&quot;软件定义的网络&quot;，我无法在生产系统上及时识别，这是我的噩梦，我不知道如何减少这种风险。<p>你的Kubernetes噩梦是什么？<p>编辑：形成\n",
  "description_zh-Hant": "所有自我託管的東西都有它的維護稅，但為什麼Kubernetes（尤其是自我託管）這麼難？是什麼讓Kubernetes的運營如此艱難？<p>- 是網絡模型，從消費的角度看很簡單，但有太多的移動部件，無法實施？<p>- 是存儲模型，CSI和朋友？<p>- 是一堆控制器循環做自己的事情，沒有任何東西可以提供一個&quot;健康&quot;的圖片來識別根本原因？<p>對我個人來說，我想到的首要事情是網絡細節。它們是由每個 CNI 解決方案以略微不同的方式和結構（iptables、虛擬網橋、路由守護程序、eBPF 等等等）自動生成的，由於它們是生成的，所以在單個節點和/或類似配置上發現數百條iptable規則和鏈的情況並不罕見。 <p>由於是自動化的，這些解決方案產生了大量的組件/配置，在出現問題的情況下，即使一個人掌握了這些組件，也需要一些時間來通過所有的組件（虛擬接口、虛擬橋、iptable鏈和規則、ipvs條目等）來識別導致問題的原因。從本質上講，一個人幾乎必須是一個網絡工程師，因為除了底層/物理（或虛擬，我指的是雲VPC）網絡，k8s在軟件/配置層實現了自己的網絡（pod網絡，集群網絡），必須完全理解才能維護。 <p>上帝保佑，如果CNI解決方案有一些邊緣情況或其他錯誤配置，它不斷產生不充分或錯誤配置的規則/路由等，導致一個破碎的&quot;軟件定義的網絡&quot;，我無法在生產系統上及時識別，這是我的噩夢，我不知道如何減少這種風險。<p>你的Kubernetes噩夢是什麼？<p>編輯：形成\n"
}