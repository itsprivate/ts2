{
  "@type": "NewsArticle",
  "identifier": "2022--06--08--en--myfeed--HackerNews--NewsArticle--31666602",
  "url": "https://news.ycombinator.com/item?id=31666602",
  "headline": "Nebulgym, a new open-source that accelerates AI training (~1.5-2x)",
  "publisher": {
    "@type": "Organization",
    "name": "HackerNews",
    "url": "https://news.ycombinator.com",
    "logo": "https://hn.buzzing.cc/avatar.png"
  },
  "description": "Training always takes too long. If it takes an hour, it would be better if it took 30 minutes, or maybe 15 minutes... or just 1 minute, why not? And if you want to speed up training, the techs available usually require to increase the complexity of the training process, whether it's making trade-off in terms of accuracy or time for the developer to learn a new framework. Often times it's trial and error, playing with parameters, training recipes, or switching framework/model. That's definitely not ideal.<p>“Fast &amp; easy-to-use” These were keywords that motivated me to work on a new way of doing training, the library nebulgym, which now is open-source (https://github.com/nebuly-ai/nebulgym).<p>FAST<p>Training should be fast, period. Wouldn't it be great if in the near future you could train a GPT3 from scratch on your laptop? Or a large EfficientNet in a fraction of a minute? Nebulgym was built to try to bring developers closer to that future. This open-source optimizes the full training computation stack, from efficient data loading to faster forward and backward passes and earlier convergence. For example, by saving data samples in the cache on the first data read, it speeds up the full data loading process and eliminates what can become the bottleneck for the training process. Nebulgym also leverages techniques such as partial compilation of some calculations and smart sparse gradients to speed up forward and backward gradient propagations. And many more features will be implemented soon. And please let me know / open issues if you have ideas for making nebulgym even faster :)<p>EASY-TO-USE<p>&quot;Not another framework, please, there're already 1000&quot;. That's a call for help from many developers, so nebulgym has been developed with this in mind. Nebulgym let you use the training setup you've always used, and works &quot;on top&quot;. This is made possible with the use of class decorators (like Java's annotations). In short, you can just add these decorators before defining the model classes, and nebulgym will make sure that you use your computing resources to the fullest.<p>Here's a snippet of training with nebulgym decorators (@accelerate_dataset and @accelerate_model)<p>--------------------------<p>import torch<p>from nebulgym.decorators.torch_decorators import \naccelerate_model, accelerate_dataset<p>@accelerate_dataset()<p>class MyDataset(torch.utils.data.Dataset):\n   #Your Dataset definition<p>@accelerate_model()<p>class MyModel(torch.nn.Module):\n   # Your model definition<p># Train your model as you usually do<p>--------------------------<p>And that's it. Give it a try, and leave a star , it's a little contribution to show some love for open-source projects :) Also feedback would be super appreciated!<p>https://github.com/nebuly-ai/nebulgym",
  "keywords": [],
  "author": {
    "@type": "Person",
    "name": "emilec___",
    "url": "https://news.ycombinator.com/user?id=emilec___"
  },
  "discussionUrl": "https://news.ycombinator.com/item?id=31666602",
  "sameAs": "https://news.ycombinator.com/item?id=31666602",
  "dateCreated": "2022-06-08T13:46:44.234Z",
  "datePublished": "2022-06-08T12:59:16.000Z",
  "dateModified": "2022-06-08T13:46:44.234Z",
  "interactionStatistic": [
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "LikeAction"
      },
      "userInteractionCount": 3
    },
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "CommentAction"
      },
      "userInteractionCount": 1
    }
  ],
  "headline_zh-Hans": "Nebulgym，一个新的开源，可以加速AI训练（~1.5-2倍）。\n",
  "headline_zh-Hant": "Nebulgym，一個新的開源，可以加速AI訓練（~1.5-2倍）。\n",
  "@context": [
    "https://schema.org",
    {
      "@vocab": "http://schema.org/",
      "@language": "en",
      "headline_zh-Hans": {
        "@id": "headline",
        "@language": "zh-Hans"
      },
      "headline_zh-Hant": {
        "@id": "headline",
        "@language": "zh-Hant"
      },
      "@version": 1.1,
      "description_zh-Hans": {
        "@id": "description",
        "@language": "zh-Hans"
      },
      "description_zh-Hant": {
        "@id": "description",
        "@language": "zh-Hant"
      }
    }
  ],
  "description_zh-Hans": "训练总是需要太长时间。如果需要一个小时，那么最好是30分钟，或者15分钟......或者只是1分钟，为什么不呢？而如果你想加快培训速度，现有的技术通常需要增加培训过程的复杂性，无论是在准确性方面还是在开发人员学习新框架的时间方面做出权衡。很多时候，它是试错，玩参数，训练配方，或切换框架/模型。<p>\"快速&amp；易于使用 \"这些关键词促使我致力于开发一种新的训练方式，即nebulgym库，它现在是开源的（https://github.com/nebuly-ai/nebulgym）。<p>快速<p>训练应该是快速的，期间。如果在不久的将来，你可以在你的笔记本电脑上从头开始训练一个GPT3，那不是很好吗？或者在一分钟内完成大型EfficientNet的训练？Nebulgym的建立就是为了让开发者更接近这个未来。这个开源项目优化了整个训练计算栈，从高效的数据加载到更快的前向和后向传递以及更早的收敛。例如，通过在第一次读取数据时将数据样本保存在缓存中，它加快了整个数据加载过程，消除了可能成为训练过程瓶颈的因素。Nebulgym还利用某些计算的部分编译和智能稀疏梯度等技术，加快梯度的前向和后向传播。还有更多的功能将很快实现。如果你有让nebulgym变得更快的想法，请让我知道/开放问题:)<p>简单易用<p>&quot;请不要使用另一个框架，已经有1000个&quot;。这是许多开发者的呼声，所以nebulgym的开发也考虑到了这一点。Nebulgym让你使用你一直使用的训练设置，并在上面工作&quot;。这是通过使用类装饰器（像Java的注解）实现的。简而言之，你只需在定义模型类之前添加这些装饰器，nebulgym将确保你充分利用你的计算资源。<p>下面是使用nebulgym装饰器（@accelerate_dataset和@accelerate_model）训练的一个片段<p>--------------------------<p>import torch<p>from nebulgym.decorators.torch_decorators import \naccelerate_model, accelerate_dataset<p>@accelerate_dataset()<p>class MyDataset(Torch.utils.data.Dataset)。\n   #你的数据集定义<p>@accelerate_model()<p>class MyModel(torch.nn.Module)。\n   # 你的模型定义<p># 像你平时那样训练你的模型<p>--------------------------<p> 就这样了。试一试，并留下一颗星，这是对开源项目的一点贡献:) 同时，我们也非常感谢你的反馈！<p>https://github.com/nebuly-ai/nebulgym\n",
  "description_zh-Hant": "訓練總是需要太長時間。如果需要一個小時，那麼最好是30分鐘，或者15分鐘......或者只是1分鐘，為什麼不呢？而如果你想加快培訓速度，現有的技術通常需要增加培訓過程的複雜性，無論是在準確性方面還是在開發人員學習新框架的時間方面做出權衡。很多時候，它是試錯，玩參數，訓練配方，或切換框架/模型。<p>\"快速&amp；易於使用 \"這些關鍵詞促使我致力於開發一種新的訓練方式，即nebulgym庫，它現在是開源的（https://github.com/nebuly-ai/nebulgym）。<p>快速<p>訓練應該是快速的，期間。如果在不久的將來，你可以在你的筆記本電腦上從頭開始訓練一個GPT3，那不是很好嗎？或者在一分鐘內完成大型EfficientNet的訓練？Nebulgym的建立就是為了讓開發者更接近這個未來。這個開源項目優化了整個訓練計算棧，從高效的數據加載到更快的前向和後向傳遞以及更早的收斂。例如，通過在第一次讀取數據時將數據樣本保存在緩存中，它加快了整個數據加載過程，消除了可能成為訓練過程瓶頸的因素。Nebulgym還利用某些計算的部分編譯和智能稀疏梯度等技術，加快梯度的前向和後向傳播。還有更多的功能將很快實現。如果你有讓nebulgym變得更快的想法，請讓我知道/開放問題:)<p>簡單易用<p>&quot;請不要使用另一個框架，已經有1000個&quot;。這是許多開發者的呼聲，所以nebulgym的開發也考慮到了這一點。Nebulgym讓你使用你一直使用的訓練設置，並在上面工作&quot;。這是通過使用類裝飾器（像Java的註解）實現的。簡而言之，你只需在定義模型類之前添加這些裝飾器，nebulgym將確保你充分利用你的計算資源。<p>下面是使用nebulgym裝飾器（@accelerate_dataset和@accelerate_model）訓練的一個片段<p>--------------------------<p>import torch<p>from nebulgym.decorators.torch_decorators import \naccelerate_model, accelerate_dataset<p>@accelerate_dataset()<p>class MyDataset(Torch.utils.data.Dataset)。\n   #你的數據集定義<p>@accelerate_model()<p>class MyModel(torch.nn.Module)。\n   # 你的模型定義<p># 像你平時那樣訓練你的模型<p>--------------------------<p> 就這樣了。試一試，並留下一顆星，這是對開源項目的一點貢獻:) 同時，我們也非常感謝你的反饋！<p>https://github.com/nebuly-ai/nebulgym\n"
}