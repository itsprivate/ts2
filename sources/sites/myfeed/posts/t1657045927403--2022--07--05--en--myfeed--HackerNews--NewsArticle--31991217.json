{
  "@type": "NewsArticle",
  "identifier": "2022--07--05--en--myfeed--HackerNews--NewsArticle--31991217",
  "url": "https://news.ycombinator.com/item?id=31991217",
  "headline": "Open-Source LaMDA Model",
  "publisher": {
    "@type": "Organization",
    "name": "HackerNews",
    "url": "https://news.ycombinator.com",
    "logo": "https://hn.buzzing.cc/avatar.png"
  },
  "description": "An open-source implementation for the pre-training architecture of Google's LaMDA in PyTorch. The research paper outlines an autoregressive, decoder-only, GPT-like transformer language model. The transformer uses T5 relative positional bias in the attention layers and gated-GELU activation function in the feed-forward layers.<p>The repository currently contains a script for basic training as well as Huggingface datasets and Weights &amp; Biases integration.<p>LaMDA research paper: https://arxiv.org/abs/2201.08239<p>Github repository for the model: https://github.com/conceptofmind/LaMDA-pytorch<p>The pre-training architecture was peer-reviewed by Dr. Phil Wang. Please check out and support his work: https://github.com/lucidrains.<p>Updates: https://twitter.com/EnricoShippole",
  "keywords": [],
  "author": {
    "@type": "Person",
    "name": "EnricoShippole",
    "url": "https://news.ycombinator.com/user?id=EnricoShippole"
  },
  "discussionUrl": "https://news.ycombinator.com/item?id=31991217",
  "sameAs": "https://news.ycombinator.com/item?id=31991217",
  "dateCreated": "2022-07-05T18:32:07.403Z",
  "datePublished": "2022-07-05T17:35:01.000Z",
  "dateModified": "2022-07-05T18:32:07.403Z",
  "interactionStatistic": [
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "LikeAction"
      },
      "userInteractionCount": 4
    },
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "CommentAction"
      },
      "userInteractionCount": 0
    }
  ],
  "headline_zh-Hans": "开源的LaMDA模型\n",
  "headline_zh-Hant": "開源的LaMDA模型\n",
  "@context": [
    "https://schema.org",
    {
      "@vocab": "http://schema.org/",
      "@language": "en",
      "headline_zh-Hans": {
        "@id": "headline",
        "@language": "zh-Hans"
      },
      "headline_zh-Hant": {
        "@id": "headline",
        "@language": "zh-Hant"
      },
      "@version": 1.1,
      "description_zh-Hans": {
        "@id": "description",
        "@language": "zh-Hans"
      },
      "description_zh-Hant": {
        "@id": "description",
        "@language": "zh-Hant"
      }
    }
  ],
  "description_zh-Hans": "在PyTorch中对谷歌的LaMDA的预训练架构进行了开源实现。该研究论文概述了一个自回归的、仅有解码器的、类似GPT的转化器语言模型。该转化器在注意层中使用T5相对位置偏差，在前馈层中使用门控-GELU激活函数。<p>目前，该存储库包含一个基本训练的脚本，以及Huggingface数据集和Weights &amp; Biases整合。<p>LaMDA研究论文：https://arxiv.org/abs/2201.08239<p>模型的Github存储库：https://github.com/conceptofmind/LaMDA-pytorch<p>该预训练架构由Phil Wang博士进行同行评审。请查看并支持他的工作：https://github.com/lucidrains。<p>更新内容：https://twitter.com/EnricoShippole\n",
  "description_zh-Hant": "在PyTorch中對谷歌的LaMDA的預訓練架構進行了開源實現。該研究論文概述了一個自迴歸的、僅有解碼器的、類似GPT的轉化器語言模型。該轉化器在注意層中使用T5相對位置偏差，在前饋層中使用門控-GELU激活函數。<p>目前，該存儲庫包含一個基本訓練的腳本，以及Huggingface數據集和Weights &amp; Biases整合。<p>LaMDA研究論文：https://arxiv.org/abs/2201.08239<p>模型的Github存儲庫：https://github.com/conceptofmind/LaMDA-pytorch<p>該預訓練架構由Phil Wang博士進行同行評審。請查看並支持他的工作：https://github.com/lucidrains。<p>更新內容：https://twitter.com/EnricoShippole\n"
}