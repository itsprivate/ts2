{
  "@type": "NewsArticle",
  "identifier": "2022--04--10--en--askhn--HackerNews--NewsArticle--30975887",
  "url": "https://news.ycombinator.com/item?id=30975887",
  "headline": "Ask HN: Why PyTorch einsum is significantly slower than transpose",
  "publisher": {
    "@type": "Organization",
    "name": "HackerNews",
    "url": "https://news.ycombinator.com",
    "logo": "https://hn.buzzing.cc/avatar.png"
  },
  "description": "I have been tinkering with some DL models and wanted to implement part of it using PyTorch einsum. Before doing so I was wondering about the performance. I have been a bit skeptic as I believe there is a parsing (and even may be somewhat code generation) involved in implementation of einsum (I have never look under the hood of PyTorch or Numpy as to how is it implemented, so I may be completely wrong)<p>So to measure the performance, I created a simple benchmark of comparison. I created a Tensor with these dimensions (BATCH, X, Y). Like so -<p>a = torch.randn(10, 20, 30)<p>Then in Jupyter I did this<p>%%timeit<p>torch.einsum('b i j -&gt; b j i', a)<p>AND<p>%%timeit<p>a.transpose(1, 2)<p>-----------------------<p>This is the result<p>5.43 µs ± 63.5 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each) [Einsum]<p>1.15 µs ± 2.51 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each) [transpose]<p>Am I doing / reading something wrong? Is it a wrong way to benchmark? Or is it really true what I see, that einsum is order of magnitude slower than transpose?",
  "keywords": [
    "Ask HN"
  ],
  "genre": "Ask HN",
  "author": {
    "@type": "Person",
    "name": "rcshubhadeep",
    "url": "https://news.ycombinator.com/user?id=rcshubhadeep"
  },
  "discussionUrl": "https://news.ycombinator.com/item?id=30975887",
  "sameAs": "https://news.ycombinator.com/item?id=30975887",
  "dateCreated": "2022-04-10T10:43:53.365Z",
  "datePublished": "2022-04-10T10:32:49.000Z",
  "dateModified": "2022-04-10T10:43:53.365Z",
  "interactionStatistic": [
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "LikeAction"
      },
      "userInteractionCount": 1
    },
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "CommentAction"
      },
      "userInteractionCount": 0
    }
  ],
  "headline_zh-Hans": "Ask HN: 为什么PyTorch的einsum明显比transpose慢？",
  "headline_zh-Hant": "Ask HN: 為什麼PyTorch的einsum明顯比transpose慢？",
  "@context": [
    "https://schema.org",
    {
      "@vocab": "http://schema.org/",
      "@language": "en",
      "headline_zh-Hans": {
        "@id": "headline",
        "@language": "zh-Hans"
      },
      "headline_zh-Hant": {
        "@id": "headline",
        "@language": "zh-Hant"
      },
      "@version": 1.1,
      "description_zh-Hans": {
        "@id": "description",
        "@language": "zh-Hans"
      },
      "description_zh-Hant": {
        "@id": "description",
        "@language": "zh-Hant"
      }
    }
  ],
  "description_zh-Hans": "我一直在修补一些DL模型，想用PyTorch einsum来实现其中的一部分。在这样做之前，我想知道性能如何。我一直有点怀疑，因为我相信在实现einsum的过程中会涉及到解析（甚至可能会有一些代码生成）（我从来没有看过PyTorch或Numpy的引擎盖下是如何实现的，所以我可能是完全错误的）<p>所以为了衡量性能，我创建了一个简单的比较基准。我创建了一个具有这些维度（BATCH、X、Y）的张量。像这样 -<p>a = torch.randn(10, 20, 30)<p>然后在Jupyter中我这样做<p>%%timeit<p>torch.einsum('b i j -&gt; b j i', a)<p>AND<p>%%timeit<p>a.transpose(1, 2)<p>-----------------------<p>这就是结果<p>5. 43 µs ± 63.5 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each) [Einsum]<p>1.15 µs ± 2.51 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each) [transpose]<p>我是不是做/读错了什么？这是一个错误的基准测试方法吗？还是真的如我所见，einsum比transpose慢了一个数量级？",
  "description_zh-Hant": "我一直在修補一些DL模型，想用PyTorch einsum來實現其中的一部分。在這樣做之前，我想知道性能如何。我一直有點懷疑，因為我相信在實現einsum的過程中會涉及到解析（甚至可能會有一些代碼生成）（我從來沒有看過PyTorch或Numpy的引擎蓋下是如何實現的，所以我可能是完全錯誤的）<p>所以為了衡量性能，我創建了一個簡單的比較基準。我創建了一個具有這些維度（BATCH、X、Y）的張量。像這樣 -<p>a = torch.randn(10, 20, 30)<p>然後在Jupyter中我這樣做<p>%%timeit<p>torch.einsum('b i j -&gt; b j i', a)<p>AND<p>%%timeit<p>a.transpose(1, 2)<p>-----------------------<p>這就是結果<p>5. 43 µs ± 63.5 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each) [Einsum]<p>1.15 µs ± 2.51 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each) [transpose]<p>我是不是做/讀錯了什麼？這是一個錯誤的基準測試方法嗎？還是真的如我所見，einsum比transpose慢了一個數量級？"
}