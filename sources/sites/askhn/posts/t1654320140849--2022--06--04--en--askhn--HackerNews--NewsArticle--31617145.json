{
  "@type": "NewsArticle",
  "identifier": "2022--06--04--en--askhn--HackerNews--NewsArticle--31617145",
  "url": "https://news.ycombinator.com/item?id=31617145",
  "headline": "Ask HN: How should I handle willful misconduct around analytics and sales?",
  "publisher": {
    "@type": "Organization",
    "name": "HackerNews",
    "url": "https://news.ycombinator.com",
    "logo": "https://hn.buzzing.cc/avatar.png"
  },
  "description": "Six-months ago I started working on an analytics team at a 200-person, private tech company. I’ve been witnessing some disturbing behavior and could use suggestions on what to do.<p>Some background:<p>The biggest internal stakeholder of our analytics team is the sales team. The CGO / CSO is hands on, joins all of our analytics-sales team meetings, and outranks the head of our analytics team, who is also present during all of our meetings.<p>Our company sells a B2B product. The bulk of our analytics projects involve running analyses to show that this product works in the particular customer context (sorry for the vagueness, trying to be anonymous here). The sales team presents these analyses to these customers to convince them to buy it.<p>We belong in an industry where it seems that customers infrequently conduct their own analyses or experiments to verify that products (like ours) works. So, weirdly to me, customers tend to just trust the analytics we present.<p>That’s the set-up. Now the troubling parts:<p>The sales team and analytics teams have tried running A/B tests in the past to demonstrate product value, but the results showed the product was of no benefit on the single, key metric of interest. Since then, the sales team has discouraged the use of A/B tests because “we don’t understand the results.” The head of the analytics team goes along with that.<p>We run observational, causal analyses to demonstrate an effect and the raw results vary from weak negative effect, to no effect, to weak positive effect. It’s a noisy analysis.<p>The few times one of our external customers has tried to A/B test the product in a trial run before fully buying it, the results have always been null.<p>To my eyes, all of the above averages out to *the product does not work*.<p>Whenever we find a negative or null effect in an analysis, the sales team pressures us into re-running the analysis looking at different sub-groups until we find a positive effect / “good story for the client”. They do this in sometimes subtle ways (e.g. “I think our data isn’t capturing a subtlety in this customer’s industry”) to more obvious pressure (e.g. “look, we have a client meeting on Friday. We need a good story. Try looking at it X way or Y way in the data). I have ample email evidence of this.<p>I’ve tried explaining to both teams that it is bad statistical practice (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4840791/) to do multiple testing / fishing / p-hacking to find good stories, but no one seems to understand this or perhaps care. So we keep doing this practice, and the sales team finds a way to spin it into a narrative for the customer.<p>I’ve spoken with the head of the analytics team, the HR team, and even the CEO about the situation. I’m diplomatic, so in all of these conversations I’ve made sure to not heap blame on individuals but calmly insist that the system is at fault and provide suggestions on how to fix it. In all of these cases, people give me platitudes about the need for integrity, thank me for speaking up, but literally nothing happens.<p>I’m still at the company but currently actively looking around for new jobs. What is an employee supposed to do in this situation? It’s a private company so there’s no whistleblowing to be done, I imagine. Would it be right to secretly inform our customers of the situation? Am I being overly dramatic?",
  "keywords": [
    "Ask HN"
  ],
  "genre": "Ask HN",
  "author": {
    "@type": "Person",
    "name": "venice_dev",
    "url": "https://news.ycombinator.com/user?id=venice_dev"
  },
  "discussionUrl": "https://news.ycombinator.com/item?id=31617145",
  "sameAs": "https://news.ycombinator.com/item?id=31617145",
  "dateCreated": "2022-06-04T05:22:20.849Z",
  "datePublished": "2022-06-04T04:57:34.000Z",
  "dateModified": "2022-06-04T05:22:20.849Z",
  "interactionStatistic": [
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "LikeAction"
      },
      "userInteractionCount": 1
    },
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "CommentAction"
      },
      "userInteractionCount": 0
    }
  ],
  "headline_zh-Hans": "Ask HN: 我应该如何处理围绕分析和销售的故意不当行为？\n",
  "headline_zh-Hant": "Ask HN: 我應該如何處理圍繞分析和銷售的故意不當行為？\n",
  "@context": [
    "https://schema.org",
    {
      "@vocab": "http://schema.org/",
      "@language": "en",
      "headline_zh-Hans": {
        "@id": "headline",
        "@language": "zh-Hans"
      },
      "headline_zh-Hant": {
        "@id": "headline",
        "@language": "zh-Hant"
      },
      "@version": 1.1,
      "description_zh-Hans": {
        "@id": "description",
        "@language": "zh-Hans"
      },
      "description_zh-Hant": {
        "@id": "description",
        "@language": "zh-Hant"
      }
    }
  ],
  "description_zh-Hans": "六个月前，我开始在一家200人的私营科技公司的分析团队工作。我目睹了一些令人不安的行为，希望能得到一些建议。<p>一些背景：<p>我们分析团队的最大内部利益相关者是销售团队。CGO/CSO亲力亲为，参加我们所有的分析-销售团队会议，而且级别高于我们分析团队的负责人，后者也出席我们所有的会议。我们的大部分分析项目涉及运行分析，以表明该产品在特定的客户背景下是有效的（抱歉，在此尝试匿名）。销售团队向这些客户展示这些分析，以说服他们购买。<p>在我们这个行业中，客户似乎很少进行自己的分析或实验来验证产品（如我们的产品）是否有效。所以，对我来说，奇怪的是，客户往往只是相信我们提出的分析。<p>这就是设置的问题。现在是令人不安的部分：<p>销售团队和分析团队过去曾尝试进行A/B测试，以证明产品的价值，但结果显示该产品在单一的关键指标上没有好处。从那时起，销售团队就不鼓励使用A/B测试，因为 \"我们不了解结果\"。分析团队的负责人对此表示赞同。<p>我们进行观察性的因果分析，以证明效果，而原始结果从微弱的负面效果，到无效果，再到微弱的正面效果不等。<p>我们的一个外部客户在完全购买产品之前，曾有几次试图在试运行中对产品进行A/B测试，结果总是无效。<p>在我看来，上述所有情况平均起来就是*产品不起作用*。<p>每当我们在分析中发现负面或无效的效果时，销售团队就会给我们施加压力，让我们重新进行分析，观察不同的子群体，直到我们找到一个积极的效果/\"对客户有利的故事\"。他们这样做的方式有时很微妙（例如，\"我认为我们的数据没有捕捉到这个客户行业的一个微妙之处\"），有时则是更明显的压力（例如，\"看，我们在周五有一个客户会议。我们需要一个好的故事。试着在数据中以X方式或Y方式看待它）。我有充分的电子邮件证据。<p>我已经尝试向两个团队解释，做多次测试/钓鱼/p-hacking来寻找好的故事是不好的统计实践（https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4840791/），但似乎没有人理解这一点，或者也许不在乎。所以我们一直在做这种做法，而销售团队则想办法把它变成对客户的叙述。<p>我已经与分析团队的负责人、人力资源团队，甚至是首席执行官谈过这个情况。我是个外交家，所以在所有这些谈话中，我都确保不把责任归咎于个人，而是冷静地坚持认为是系统出了问题，并就如何解决这个问题提供建议。在所有这些情况下，人们都对我说了一些需要诚信的陈词滥调，感谢我的发言，但实际上什么都没有发生。<p>我还在公司，但目前正在积极寻找新的工作。在这种情况下，员工应该怎么做？这是一家私人公司，所以我想，没有什么可以举报的。秘密地将这种情况告知我们的客户，这样做对吗？我是不是太夸张了？\n",
  "description_zh-Hant": "六個月前，我開始在一家200人的私營科技公司的分析團隊工作。我目睹了一些令人不安的行為，希望能得到一些建議。<p>一些背景：<p>我們分析團隊的最大內部利益相關者是銷售團隊。CGO/CSO親力親為，參加我們所有的分析-銷售團隊會議，而且級別高於我們分析團隊的負責人，後者也出席我們所有的會議。我們的大部分分析項目涉及運行分析，以表明該產品在特定的客戶背景下是有效的（抱歉，在此嘗試匿名）。銷售團隊向這些客戶展示這些分析，以說服他們購買。<p>在我們這個行業中，客戶似乎很少進行自己的分析或實驗來驗證產品（如我們的產品）是否有效。所以，對我來說，奇怪的是，客戶往往只是相信我們提出的分析。<p>這就是設置的問題。現在是令人不安的部分：<p>銷售團隊和分析團隊過去曾嘗試進行A/B測試，以證明產品的價值，但結果顯示該產品在單一的關鍵指標上沒有好處。從那時起，銷售團隊就不鼓勵使用A/B測試，因為 \"我們不瞭解結果\"。分析團隊的負責人對此表示贊同。<p>我們進行觀察性的因果分析，以證明效果，而原始結果從微弱的負面效果，到無效果，再到微弱的正面效果不等。<p>我們的一個外部客戶在完全購買產品之前，曾有幾次試圖在試運行中對產品進行A/B測試，結果總是無效。<p>在我看來，上述所有情況平均起來就是*產品不起作用*。<p>每當我們在分析中發現負面或無效的效果時，銷售團隊就會給我們施加壓力，讓我們重新進行分析，觀察不同的子群體，直到我們找到一個積極的效果/\"對客戶有利的故事\"。他們這樣做的方式有時很微妙（例如，\"我認為我們的數據沒有捕捉到這個客戶行業的一個微妙之處\"），有時則是更明顯的壓力（例如，\"看，我們在週五有一個客戶會議。我們需要一個好的故事。試著在數據中以X方式或Y方式看待它）。我有充分的電子郵件證據。<p>我已經嘗試向兩個團隊解釋，做多次測試/釣魚/p-hacking來尋找好的故事是不好的統計實踐（https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4840791/），但似乎沒有人理解這一點，或者也許不在乎。所以我們一直在做這種做法，而銷售團隊則想辦法把它變成對客戶的敘述。<p>我已經與分析團隊的負責人、人力資源團隊，甚至是首席執行官談過這個情況。我是個外交家，所以在所有這些談話中，我都確保不把責任歸咎於個人，而是冷靜地堅持認為是系統出了問題，並就如何解決這個問題提供建議。在所有這些情況下，人們都對我說了一些需要誠信的陳詞濫調，感謝我的發言，但實際上什麼都沒有發生。<p>我還在公司，但目前正在積極尋找新的工作。在這種情況下，員工應該怎麼做？這是一傢俬人公司，所以我想，沒有什麼可以舉報的。秘密地將這種情況告知我們的客戶，這樣做對嗎？我是不是太誇張了？\n"
}