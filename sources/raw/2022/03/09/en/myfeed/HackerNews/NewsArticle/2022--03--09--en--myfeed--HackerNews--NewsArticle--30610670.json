{
  "@type": "NewsArticle",
  "identifier": "2022--03--09--en--myfeed--HackerNews--NewsArticle--30610670",
  "url": "https://news.ycombinator.com/item?id=30610670",
  "headline": "How do you version your large files for machine learning?",
  "publisher": {
    "@type": "Organization",
    "name": "HackerNews",
    "url": "https://news.ycombinator.com",
    "logo": "https://hn.buzzing.cc/avatar.png"
  },
  "description": "Just tar your data and put it in s3? use a shared folder in NFS? or use git-like solutions, like DVC, Git LFS?<p>Undoubtedly, putting large files in s3 (or similar object store) or NFS is the most common solution. However, when it comes to version control, Git is the defacto solution. But Git is not designed for versioning large files. If we want to put large files on S3, why donâ€™t we just use a tool that does the data versioning just on top of S3? This is the motivation why we develop the ArtiV(Artifact Versions), a version control tool for large files.<p>For more detail, I recommend you to read this blog post \nhttps://blog.infuseai.io/a-modern-approach-to-versioning-large-datasets-for-machine-learning-fca2f541dd85<p>Or check out our git repository https://github.com/InfuseAI/artiv",
  "keywords": [],
  "author": {
    "@type": "Person",
    "name": "popcornylu",
    "url": "https://news.ycombinator.com/user?id=popcornylu"
  },
  "discussionUrl": "https://news.ycombinator.com/item?id=30610670",
  "sameAs": "https://news.ycombinator.com/item?id=30610670",
  "dateCreated": "2022-03-09T03:49:55.685Z",
  "datePublished": "2022-03-09T03:42:58.000Z",
  "dateModified": "2022-03-09T03:49:55.685Z",
  "interactionStatistic": [
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "LikeAction"
      },
      "userInteractionCount": 2
    },
    {
      "@type": "InteractionCounter",
      "interactionType": {
        "@type": "CommentAction"
      },
      "userInteractionCount": 0
    }
  ]
}